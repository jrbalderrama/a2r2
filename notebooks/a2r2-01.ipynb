{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrbalderrama/a2r2/blob/main/a2r2-01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUDI Workshop: Introduction to Privacy-Preserving Data Publishing Techniques\n",
    "\n",
    "Tristan ALLARD & Javier ROJAS BALDERRAMA\n",
    "\n",
    "_Univ Rennes, CNRS, INRIA_\n",
    "  \n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)"
   ],
   "metadata": {
    "id": "5qOVUmdqyTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Acknowledgments\n",
    "\n",
    "We warmly thank François Bodin and Luc Lesoil for their support on the data and the definition of the use-case.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook __ONE__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 0 (STARTER)\n",
    "\n",
    "<a id='step_0'></a>\n",
    "\n",
    "This hands-on tutorial is going to introduce you to the issue of *privacy-preserving personal data publishing*. You are going to follow the implementation of a concrete use-case built from open data from the Rennes Metropole area. The main question of the use-case is to know wether a change in the students schedules at the Beaulieu campus impacts the load of the buses that go through the campus. We will answer to this question based on two datasets : the validations inside the buses that stop close to the campus (with timestamps), and the number of students that terminate a class (with timestamps). Our approach consists in training a predictor that outputs the expected number of validations along the day given the number of students terminating a class along the day. However, using raw buses validations for answering to this question may lead to privacy issues because validations can be highly identifying. After having performed some reidentification attacks, you will use a perturbed version of the buses validations dataset and observe the resulting impact on our ability to answer to the main question of the use-case.\n",
    "\n",
    "We designed this tutorial to be a step-by-step guided tour. You can follow sequentially the \"Step i\" tag inside the titles of the sections. Up to you to follow the sequence proposed or to deviate from it, but be careful when leaving the track, it's wild out there ;)\n",
    "\n",
    "We divided the full journey into three topics:\n",
    "\n",
    "1. The naive version\n",
    "2. Privacy issues\n",
    "3. The protected version\n",
    "\n",
    "For your convenience, there are a dedicated notebook for each topic, **you are currently in Notebook ONE**.\n",
    "\n",
    "The notebooks also include questions. Please take some time to think about them. Trying to answer these questions can also help you to gain a deeper understanding. And we would love reading your answers!\n",
    "\n",
    "Ready?\n",
    "\n",
    "Really??\n",
    "\n",
    "Please run the whole notebook (it does not take long) and **go directly to the [Step 1](#step_1).**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2 (PREAMBLE): Settings and datasets\n",
    "\n",
    "<a id='step_2'></a>\n",
    "\n",
    "Not too disappointed ? So lets now have a look at the data based on which we trained the model. \n",
    "\n",
    "1. The datasets are downloaded\n",
    "2. The libraries required are imported and global variables are setup\n",
    "3. The raw data are aggregated...\n",
    "4. ... and the results are displayed.\n",
    "5. The datasets are prepared for the training process.\n",
    "\n",
    "> Observe the buses validations dataset (section [Display raw data](#sec_display_raw_data))... Can you imagine any issue?"
   ],
   "metadata": {
    "id": "NmKSCNfpfOQ-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    " ### Download datasets\n"
   ],
   "metadata": {
    "id": "qzLkzFP3fZm-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -nv -nc https://zenodo.org/record/5509313/files/classes.parquet\n",
    "!wget -nv -nc https://zenodo.org/record/5509268/files/buses.parquet"
   ],
   "outputs": [],
   "metadata": {
    "id": "u_Eju0G4yTjY",
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import required modules"
   ],
   "metadata": {
    "id": "O4teYxHrfnuh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import importlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from errno import ENOENT\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from IPython import display, get_ipython\n",
    "from numpy import ndarray\n",
    "from pandas import NA, DataFrame, DatetimeIndex, Series, Timedelta, Timestamp\n",
    "from plotly import subplots\n",
    "from plotly.graph_objs import Bar, Candlestick, Figure, Scatter\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import Tensor\n",
    "from torch.nn import LSTM, Linear, Module, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "PLHjpQH6yTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup notebook constants and running environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# project base directory\n",
    "BASE_DIRECTORY = Path(\".\")\n",
    "\n",
    "# detect running environment\n",
    "COLAB_ON = True if \"google.colab\" in str(get_ipython()) else False"
   ],
   "outputs": [],
   "metadata": {
    "id": "82e_w9dyyTjY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set Ploty renderer\n",
    "if COLAB_ON:\n",
    "    pio.renderers.default = \"colab\""
   ],
   "outputs": [],
   "metadata": {
    "id": "fToRyDS0yTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and display raw datasets"
   ],
   "metadata": {
    "id": "-kXTLBmBfxuL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Read raw data"
   ],
   "metadata": {
    "id": "CQbd6zhNR-j5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read data from file system and plot data\n",
    "\n",
    "# load dataset from file system\n",
    "def load_data(\n",
    "    path: Path,\n",
    ") -> DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(ENOENT, os.strerror(ENOENT), path)\n",
    "\n",
    "    table = pq.read_table(path)\n",
    "    return table.to_pandas()\n",
    "\n",
    "\n",
    "# buses dataset\n",
    "buses_filename = \"buses.parquet\"\n",
    "buses_path = BASE_DIRECTORY.joinpath(buses_filename)\n",
    "buses_dataset = load_data(buses_path)\n",
    "\n",
    "\n",
    "# classes dataset\n",
    "classes_filename = \"classes.parquet\"\n",
    "classes_path = BASE_DIRECTORY.joinpath(classes_filename)\n",
    "classes_dataset = load_data(classes_path)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "iUo9bi14yTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display raw data\n",
    "\n",
    "<a id='sec_display_raw_data'></a>"
   ],
   "metadata": {
    "id": "amAWRS8-RrXl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show a dataframe as a table\n",
    "def display_dataframe(\n",
    "        dataframe: DataFrame,\n",
    ") -> None:    \n",
    "    if COLAB_ON:\n",
    "        spec = importlib.util.find_spec(\"google.colab\")\n",
    "        if spec:            \n",
    "            data_table = importlib.import_module(\"google.colab.data_table\")            \n",
    "            enable_dataframe_formatter = getattr(\n",
    "                data_table, \n",
    "                \"enable_dataframe_formatter\",\n",
    "            )            \n",
    "            \n",
    "            enable_dataframe_formatter()            \n",
    "           \n",
    "    display.display(dataframe[:20000] if COLAB_ON else dataframe) \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####################\n",
    "# BEGIN : Observe"
   ],
   "outputs": [],
   "metadata": {
    "id": "XYS_XqJHR3Kj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_dataframe(buses_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Observe\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {
    "id": "-H-J9lkbTynD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####################\n",
    "# BEGIN : Answer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Food for thoughts\n",
    "\n",
    "> 1. Is there any information directly identifying in the raw data?\n",
    "> 2. Could you describe possible auxiliary information that could lead to re-identifications?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Answer\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process raw data"
   ],
   "metadata": {
    "id": "u3ZxqVvcSSgF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pre processing transportation data\n",
    "def pre_process_by_aggregation(\n",
    "    dataframe: DataFrame,\n",
    "    *,\n",
    "    stops: Optional[Sequence[str]],\n",
    "    ignore_weekend: bool = False,\n",
    ") -> DataFrame:\n",
    "\n",
    "    dataframe_ = dataframe.copy()\n",
    "    # filter data from 'bus_stops' only\n",
    "    if stops:\n",
    "        dataframe_ = dataframe_[dataframe_[\"stop_name\"].isin(beaulieu)]\n",
    "\n",
    "    # remove weekend information\n",
    "    if ignore_weekend:\n",
    "        dataframe_ = dataframe_.set_index(\"departure_time\")\n",
    "        dataframe_ = dataframe_[dataframe_.index.dayofweek < 5]\n",
    "\n",
    "    # aggregate dataset by stop name and departure time\n",
    "    dataframe_ = (\n",
    "        dataframe_.groupby(\n",
    "            [\n",
    "                \"stop_name\",\n",
    "                \"departure_time\",\n",
    "            ]\n",
    "        )\n",
    "        .agg({\"count\": \"sum\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return dataframe_.groupby(\"departure_time\").sum()"
   ],
   "outputs": [],
   "metadata": {
    "id": "VwGBHccSyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# target bus stops\n",
    "beaulieu = [\n",
    "    \"Les Préales\",\n",
    "    \"Tournebride\",\n",
    "    \"Beaulieu Chimie\",\n",
    "    \"Beaulieu INSA\",\n",
    "    \"Beaulieu Restau U\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "buses_dataset = pre_process_by_aggregation(\n",
    "    buses_dataset,\n",
    "    stops=beaulieu,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "scDTdmHFyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display agregated data"
   ],
   "metadata": {
    "id": "ZV4MJqgogTB0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show a timeseries graph of a selected attribute\n",
    "def plot_dataset(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    scatter = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[column],\n",
    "        mode=\"lines\",\n",
    "        name=\"values\",\n",
    "    )\n",
    "\n",
    "    figure.add_trace(scatter)\n",
    "    figure.update_layout(\n",
    "        showlegend=False,\n",
    "        title_text=column,\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(showgrid=True)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of validations"
   ],
   "metadata": {
    "id": "FX__8ghegg3F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_dataframe(buses_dataset)\n",
    "plot_dataset(buses_dataset, \"count\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "33PDn8bpyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of students"
   ],
   "metadata": {
    "id": "UjmGFdCDgpk3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_dataframe(classes_dataset)\n",
    "plot_dataset(classes_dataset, \"nombre_etudiant\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "OP9gJJueyTk8",
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Merge dataset together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(\n",
    "    classes: DataFrame,\n",
    "    buses: DataFrame,\n",
    ") -> DataFrame:\n",
    "\n",
    "    # ignore dataset entries that are not available in classes timeline\n",
    "    buses_ = buses[\n",
    "        buses.index\n",
    "        <= classes.index.max()\n",
    "        + Timedelta(\n",
    "            1,\n",
    "            unit=\"day\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # merge datasets\n",
    "    dataset = pd.merge(\n",
    "        classes,\n",
    "        buses_,\n",
    "        how=\"outer\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # fill empty values\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {
    "id": "8eOCLUJEyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = merge_datasets(classes_dataset, buses_dataset)\n",
    "display_dataframe(dataset)"
   ],
   "outputs": [],
   "metadata": {
    "id": "ZMIuDquXyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display dataset subsets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute a next monday after a given number of weeks for the \n",
    "# initial value (min) of the datetime index\n",
    "def get_timestamp_bound(\n",
    "    dataframe: DataFrame,\n",
    "    weeks: int,\n",
    ") -> Timestamp:\n",
    "    timedelta = Timedelta(7 * weeks - 1, unit=\"day\")\n",
    "    timestamp = dataframe.index.min() + timedelta\n",
    "    return timestamp.normalize()\n",
    "\n",
    "\n",
    "# show timeline divided bt delimiters and holidays\n",
    "def plot_timeline(\n",
    "    dataframe: DataFrame,\n",
    "    columns: Sequence[str],\n",
    "    delimiters: Sequence[Timestamp],\n",
    "    holidays: Tuple[Timestamp, Timestamp],\n",
    ") -> None:\n",
    "    dmin = dataframe[\"nombre_etudiant\"].values.min()\n",
    "    dmax = dataframe[\"nombre_etudiant\"].values.max()\n",
    "    figure = subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    for counter, column in enumerate(columns):\n",
    "        secondary_y = False if counter % 2 == 0 else True\n",
    "        scatter = Scatter(\n",
    "            x=dataframe.index,\n",
    "            y=dataframe[column],\n",
    "            mode=\"lines\",\n",
    "            name=column,\n",
    "        )\n",
    "      \n",
    "        figure.add_trace(\n",
    "            scatter,\n",
    "            secondary_y=secondary_y,\n",
    "        )\n",
    "      \n",
    "    for delimiter in delimiters:\n",
    "        figure.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=delimiter,\n",
    "            x1=delimiter,\n",
    "            y0=dmax,\n",
    "            y1=0,\n",
    "            line=dict(\n",
    "                # color=\"Gray\",\n",
    "                width=1,                 \n",
    "                dash=\"dashdot\",\n",
    "            ),    \n",
    "        )\n",
    "\n",
    "    figure.add_shape(\n",
    "        type=\"rect\",\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        layer=\"below\",\n",
    "        fillcolor=\"LightSeaGreen\",\n",
    "        x0=holidays[0],\n",
    "        x1=holidays[1],\n",
    "        y0=dmax,\n",
    "        y1=0,\n",
    "    )\n",
    "\n",
    "    figure.add_annotation(\n",
    "        x=holidays[0],\n",
    "        y=dmax,\n",
    "        align=\"right\",\n",
    "        text=\"holidays\",\n",
    "        showarrow=False,\n",
    "        yshift=-25,\n",
    "        textangle=90,\n",
    "        xshift=10,\n",
    "    )\n",
    "                                                                                    \n",
    "    figure.add_annotation(\n",
    "        x=delimiters[0],\n",
    "        y=dmax,\n",
    "        text=\"validation\",\n",
    "        showarrow=True,\n",
    "         yshift=-15,\n",
    "    )\n",
    "\n",
    "    figure.add_annotation(\n",
    "        x=delimiters[1],\n",
    "        y=dmax,\n",
    "        text=\"test\",\n",
    "        showarrow=True,\n",
    "    )\n",
    "\n",
    "    figure.update_shapes(dict(xref=\"x\", yref=\"y\"))\n",
    "    figure.update_yaxes(\n",
    "        rangemode=\"tozero\",\n",
    "        # type=\"log\",\n",
    "        )\n",
    "\n",
    "    figure.update_xaxes(range=[dataframe.index.min(), dataframe.index.max()])\n",
    "    figure.update_yaxes(title_text=columns[0], secondary_y=False)\n",
    "    figure.update_yaxes(title_text=columns[1], secondary_y=True)            \n",
    "    figure.update_layout(\n",
    "        title_text=\"Count of Buses & Classes\",\n",
    "        template=\"simple_white\",  \n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )     \n",
    "    )\n",
    "\n",
    "    figure.show()     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "la_rentree = Timestamp(\"2021-09-06\")\n",
    "la_toussaint = Timestamp(\"2021-11-01\")\n",
    "one_week_timedelta = Timedelta(7, unit=\"day\")\n",
    "\n",
    "end_train = get_timestamp_bound(dataset, weeks=9)\n",
    "end_val = get_timestamp_bound(dataset, weeks=10)\n",
    "\n",
    "plot_timeline(\n",
    "    dataset,\n",
    "    [\"nombre_etudiant\", \"count\"],\n",
    "    [end_train, end_val],\n",
    "    (la_toussaint, la_toussaint + one_week_timedelta),\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Enhance data attributes to create a predictive model based on machine learning"
   ],
   "metadata": {
    "id": "F-CFufprhULZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Improve dataset by mining information from date and time "
   ],
   "metadata": {
    "id": "TF56rcALkfO7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# bucketize attribute\n",
    "def onehot_encode(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> DataFrame:\n",
    "    dummies = pd.get_dummies(\n",
    "        dataframe[column],\n",
    "        prefix=column,\n",
    "    )\n",
    "\n",
    "    return pd.concat(\n",
    "        [dataframe, dummies],\n",
    "        axis=1,\n",
    "    ).drop(columns=[column])\n",
    "\n",
    "\n",
    "# encode (time) column as periodic wave\n",
    "def periodic_encode(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    "    period: int,\n",
    "    start_num: int = 0,\n",
    ") -> DataFrame:\n",
    "    kwargs = {\n",
    "        f\"sin_{column}\": lambda x: np.sin(\n",
    "            2 * np.pi * (dataframe[column] - start_num) / period\n",
    "        ),\n",
    "        f\"cos_{column}\": lambda x: np.cos(\n",
    "            2 * np.pi * (dataframe[column] - start_num) / period\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return dataframe.assign(**kwargs).drop(columns=[column])\n",
    "\n",
    "\n",
    "# mark dataset ranges as holidays\n",
    "def label_holidays(\n",
    "    dataframe: DataFrame,\n",
    "    start: Timestamp,\n",
    "    end: Timestamp,\n",
    "    column=\"holiday\",\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    dataframe_[column] = 0\n",
    "    dataframe_.loc[\n",
    "        (dataframe_.index >= start) & (dataframe_.index < end),\n",
    "        column,\n",
    "    ] = 1\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "# add features to the dataset\n",
    "def add_features(\n",
    "    dataframe: DataFrame,\n",
    "    bucketize_date: bool = True,\n",
    "    periodic_time: bool = True,\n",
    "    holidays: bool = False,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    if bucketize_date:\n",
    "        dataframe_ = dataframe_.assign(dayofweek=dataframe_.index.dayofweek)\n",
    "        # .assign(day=dataframe.index.day)\n",
    "        # .assign(month=dataset.index.month)\n",
    "        dataframe_ = onehot_encode(dataframe_, \"dayofweek\")\n",
    "        # dataset = onehot_encode(dataset, \"month\")\n",
    "\n",
    "    if periodic_time:\n",
    "        dataframe_ = dataframe_.assign(hour=dataframe_.index.hour)\n",
    "        dataframe_ = dataframe_.assign(minute=dataframe_.index.minute)\n",
    "        dataframe_ = periodic_encode(dataframe_, \"hour\", 24, 0)\n",
    "        dataframe_ = periodic_encode(dataframe_, \"minute\", 60, 0)\n",
    "\n",
    "    if holidays:\n",
    "        dataframe_ = label_holidays(\n",
    "            dataframe_,\n",
    "            la_toussaint,\n",
    "            la_toussaint + one_week_timedelta,\n",
    "        )\n",
    "  \n",
    "    # dataframe.drop([\"nombre_etudiant\"], axis=1, inplace=True)\n",
    "    return dataframe_"
   ],
   "outputs": [],
   "metadata": {
    "id": "CXl1HRlCyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display resulting dataset formatted for the machine learning process"
   ],
   "metadata": {
    "id": "sXBXzc0YkBwt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = add_features(dataset, holidays=True)\n",
    "display_dataframe(dataset)"
   ],
   "outputs": [],
   "metadata": {
    "id": "hLSZvIr4yTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Split the dataset to train a machine learning tool"
   ],
   "metadata": {
    "id": "CiD_2WATkqsE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split the data into test, validation, and train sets\n",
    "def features_split(\n",
    "    dataframe: DataFrame,\n",
    "    target: str,\n",
    ") -> Tuple[DataFrame, DataFrame]:\n",
    "    y = dataframe[[target]]\n",
    "    X = dataframe.drop(columns=[target])\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = dataset[dataset.index < end_train]\n",
    "val_dataset = dataset[(dataset.index >= end_train) & (dataset.index < end_val)]\n",
    "test_dataset = dataset[dataset.index >= end_val]\n",
    "\n",
    "X_train, y_train = features_split(train_dataset, target=\"count\")\n",
    "X_val, y_val = features_split(val_dataset, target=\"count\")\n",
    "X_test, y_test = features_split(test_dataset, target=\"count\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "melyhYOiACiE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TOOL: A neural network"
   ],
   "metadata": {
    "id": "b5Hiwx5O7gXw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define a neural network model\n"
   ],
   "metadata": {
    "id": "wxRu24uH8vQM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define RNN (LSTM) model\n",
    "class LSTMModel(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            layer_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(\n",
    "            self.layer_dim,\n",
    "            x.size(0),\n",
    "            self.hidden_dim,\n",
    "        ).requires_grad_()\n",
    "\n",
    "        # initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(\n",
    "            self.layer_dim,\n",
    "            x.size(0),\n",
    "            self.hidden_dim,\n",
    "        ).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        # (squeezing is equivalent to: `out = out[:, -1, :]`)\n",
    "        out = torch.squeeze(out)\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "2wjqVABVyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure the neural network"
   ],
   "metadata": {
    "id": "gX7rcuEslD4T"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# dimension (neurons) of a hidden layer \n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "# number of hidden layers\n",
    "LAYER_DIM = 3\n",
    "\n",
    "# number of rows processed at the same time\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# number of iterations during training\n",
    "EPOCHS = 100"
   ],
   "outputs": [],
   "metadata": {
    "id": "2mgESKb_yTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_dim = len(X_train.columns)  # X_train.shape[0]\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    layer_dim=LAYER_DIM,\n",
    "    output_dim=1,\n",
    "    dropout=0.2,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the neural network"
   ],
   "metadata": {
    "id": "UORhj0JelLX3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Helper to train the NN model\n",
    "class RunnerHelper:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "\n",
    "        # set model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # make predictions\n",
    "        ŷ = self.model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(ŷ, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # reset to zero gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # returns loss\n",
    "        return loss.item()\n",
    "\n",
    "    def val_step(self, X, y):\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # make prediction\n",
    "        ŷ = self.model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(ŷ, y)\n",
    "\n",
    "        # return loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, n_epochs=50):\n",
    "        model_path = f'{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_train_losses = []\n",
    "            for x_train, y_train in train_loader:\n",
    "                # x_train = x_train.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                x_train = torch.unsqueeze(x_train, 1)\n",
    "                train_loss = self.train_step(x_train, y_train)\n",
    "                batch_train_losses.append(train_loss)\n",
    "\n",
    "            training_loss = np.mean(batch_train_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    # x_val = x_val.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                    x_val = torch.unsqueeze(x_val, 1)\n",
    "                    val_loss = self.val_step(x_val, y_val)\n",
    "                    batch_val_losses.append(val_loss)\n",
    "\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 20 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch:3d}/{n_epochs}] Training loss: {training_loss:.4f}\"\n",
    "                    f\"\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                # x_test = x_test.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                x_test = torch.unsqueeze(x_test, 1)\n",
    "                self.model.eval()\n",
    "                ŷ = self.model(x_test)\n",
    "                predictions.append(ŷ.detach().numpy())\n",
    "                values.append(y_test.detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        figure = Figure()\n",
    "        tics = [*range(len(self.train_losses) + 1)]\n",
    "        value = Scatter(\n",
    "            x=tics,\n",
    "            y=self.train_losses,\n",
    "            mode=\"lines\",\n",
    "            name=\"Training\",\n",
    "            marker=dict(),\n",
    "        )\n",
    "\n",
    "        figure.add_trace(value)\n",
    "        value = Scatter(\n",
    "            x=tics,\n",
    "            y=self.val_losses,\n",
    "            mode=\"lines\",\n",
    "            name=\"Validation\",\n",
    "            marker=dict(),\n",
    "        )\n",
    "\n",
    "        figure.add_trace(value)\n",
    "        figure.update_layout(title_text=\"Losses\")\n",
    "        figure.update_xaxes(title_text=\"epoch\")\n",
    "        figure.update_yaxes(title_text=\"loss (%)\")\n",
    "        figure.show()\n",
    "\n",
    "\n",
    "# rescale results and align it to original time index\n",
    "def inverse_transform(\n",
    "    values: Sequence[ndarray],\n",
    "    predictions: Sequence[ndarray],\n",
    "    index: DatetimeIndex,\n",
    "    scaler: MinMaxScaler,\n",
    ") -> DataFrame:\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    dataframe = DataFrame(\n",
    "        data={\n",
    "            \"value\": vals,\n",
    "            \"prediction\": preds,\n",
    "        },\n",
    "        index=index[: len(vals)],\n",
    "    )\n",
    "\n",
    "    dataframe = dataframe.sort_index()\n",
    "    dataframe = DataFrame(\n",
    "        scaler.inverse_transform(dataframe),\n",
    "        columns=dataframe.columns,\n",
    "        index=dataframe.index,\n",
    "    )\n",
    "\n",
    "    return dataframe.astype(\"int_\")\n",
    "\n",
    "\n",
    "# formating data for NN\n",
    "def to_dataloaders(\n",
    "    dataframe_train: Tuple[DataFrame, DataFrame],\n",
    "    dataframe_val: Tuple[DataFrame, DataFrame],\n",
    "    dataframe_test: Tuple[DataFrame, DataFrame],\n",
    "    scaler: MinMaxScaler,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    # scale data\n",
    "    X_train_arr = scaler.fit_transform(dataframe_train[0])\n",
    "    X_val_arr = scaler.transform(dataframe_val[0])\n",
    "    X_test_arr = scaler.transform(dataframe_test[0])\n",
    "\n",
    "    y_train_arr = scaler.fit_transform(dataframe_train[1])\n",
    "    y_val_arr = scaler.transform(dataframe_val[1])\n",
    "    y_test_arr = scaler.transform(dataframe_test[1])\n",
    "\n",
    "    # transform scaled data to tensors\n",
    "    train_features = Tensor(X_train_arr)\n",
    "    train_targets = Tensor(y_train_arr)\n",
    "    val_features = Tensor(X_val_arr)\n",
    "    val_targets = Tensor(y_val_arr)\n",
    "    test_features = Tensor(X_test_arr)\n",
    "    test_targets = Tensor(y_test_arr)\n",
    "\n",
    "    # setup tensor datasets\n",
    "    train = TensorDataset(train_features, train_targets)\n",
    "    val = TensorDataset(val_features, val_targets)\n",
    "    test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    # setup (tensor) datasets loaders\n",
    "    train_loader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test,\n",
    "        batch_size=1,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = MinMaxScaler()  # RobustScaler()  # StandardScaler()  # MinMaxScaler()\n",
    "loss_fn = MSELoss()  # L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "runner = RunnerHelper(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "train_loader, val_loader, test_loader = to_dataloaders(\n",
    "    (X_train, y_train),\n",
    "    (X_val, y_val),\n",
    "    (X_test, y_test),    \n",
    "    scaler,\n",
    "    BATCH_SIZE,\n",
    ")\n",
    "\n",
    "runner.train(train_loader, val_loader, n_epochs=EPOCHS)\n",
    "runner.plot_losses()\n",
    "predictions, values = runner.evaluate(test_loader)\n",
    "lstm_result = inverse_transform(values, predictions, X_test.index, scaler)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Xr5aNd4VyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the quality of the training"
   ],
   "metadata": {
    "id": "DmxvTcwSlaqm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def print_metrics(\n",
    "    dataframe: DataFrame,\n",
    "    value: str,\n",
    "    prediction: str = \"prediction\",\n",
    ") -> None:\n",
    "    result_metrics = {\n",
    "        \"mae\": metrics.mean_absolute_error(\n",
    "            dataframe[value],\n",
    "            dataframe[prediction],\n",
    "        ),\n",
    "        \"rmse\": metrics.mean_squared_error(\n",
    "            dataframe[value],\n",
    "            dataframe[prediction],\n",
    "        )\n",
    "        ** 0.5,\n",
    "        \"r2\": metrics.r2_score(\n",
    "            dataframe[value],\n",
    "            dataframe[prediction],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(\"\\tMean Absolute Error:       \", result_metrics[\"mae\"])\n",
    "    print(\"\\tRoot Mean Squared Error:   \", result_metrics[\"rmse\"])\n",
    "    print(\"\\tR^2 Score:                 \", result_metrics[\"r2\"])\n",
    "    # return result_metrics\n",
    "\n",
    "\n",
    "# show residuals as kind of OHLC Charts\n",
    "def plot_residuals(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    hovertext = []\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        hovertext.append(\n",
    "            f\"{dataframe.index[i]}<br>\"\n",
    "            f\"Real: {dataframe['value'][i]}<br>\"\n",
    "            f\"Prediction: {dataframe['prediction'][i]}\"\n",
    "        )\n",
    "\n",
    "    figure = Figure(\n",
    "        data=[\n",
    "            Scatter(\n",
    "                x=dataframe.index,\n",
    "                y=dataframe[\"value\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"reference\",\n",
    "                line=dict(color=\"lightgrey\", width=0.6, dash=\"dot\"),\n",
    "                # opacity=0.6,\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            Scatter(\n",
    "                x=dataframe.index,\n",
    "                y=dataframe[\"prediction\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"prediction\",\n",
    "                line=dict(color=\"lightblue\", width=0.6, dash=\"dot\"),\n",
    "                showlegend=False,\n",
    "                # opacity=0.6,\n",
    "            ),\n",
    "            Candlestick(\n",
    "                x=dataframe.index,\n",
    "                open=dataframe[\"value\"],\n",
    "                high=dataframe[\"prediction\"],\n",
    "                low=dataframe[\"prediction\"],\n",
    "                close=dataframe[\"value\"],\n",
    "                text=hovertext,\n",
    "                hoverinfo=\"text\",\n",
    "                name=\"residuals\",\n",
    "                # line=dict(width=2),\n",
    "                increasing_line_color=\"lightseagreen\",\n",
    "                decreasing_line_color=\"lightsalmon\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    figure.update_layout(\n",
    "        title=\"Prediction residuals\",\n",
    "        template=\"simple_white\",\n",
    "        xaxis_rangeslider_visible=True,\n",
    "    )\n",
    "\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"NN model: LSTM\")\n",
    "print_metrics(lstm_result, \"value\")\n",
    "display_dataframe(lstm_result)\n",
    "plot_residuals(lstm_result)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Th2MSHU5lrfL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare the neural network against a baseline method"
   ],
   "metadata": {
    "id": "EaQUkM6X8_2s"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Train a linear regression model"
   ],
   "metadata": {
    "id": "k8nCaMtem6sC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build a baseline model to compare against the RNN model\n",
    "def baseline_evaluate(\n",
    "    X_train: DataFrame,\n",
    "    y_train: DataFrame,\n",
    "    X_test: DataFrame,\n",
    "    y_test: DataFrame,\n",
    ") -> DataFrame:\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    dataframe = DataFrame(y_test)\n",
    "    dataframe = dataframe.assign(prediction=prediction)\n",
    "    dataframe = dataframe.sort_index()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def plot_models_prediction_interval(\n",
    "    dataframe: DataFrame,\n",
    "    rnn_dataframe: DataFrame,\n",
    "    baseline_dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    value = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[\"count\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Reference\",\n",
    "        line=dict(color=\"rgba(0,0,0, 0.3)\", width=1, dash=\"dot\"),\n",
    "    )\n",
    "\n",
    "    figure.add_trace(value)\n",
    "    baseline = Scatter(\n",
    "        x=baseline_dataframe.index,\n",
    "        y=baseline_dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"Linear Regression\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    "\n",
    "    figure.add_trace(baseline)\n",
    "    prediction = Scatter(\n",
    "        x=rnn_dataframe.index,\n",
    "        y=rnn_dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"LSTM NN\",\n",
    "        # marker=dict(),\n",
    "        opacity=0.8,\n",
    "        visible=\"legendonly\",\n",
    "    )\n",
    "\n",
    "    figure.add_trace(prediction)\n",
    "    figure.update_layout(\n",
    "        showlegend=True,\n",
    "        title_text=\"Predictions\",\n",
    "        template=\"simple_white\",\n",
    "        xaxis=dict(\n",
    "            range=[\n",
    "                rnn_dataframe.index.min(),\n",
    "                rnn_dataframe.index.max(),\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(rangeslider_visible=True)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "pNEy9gZNyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize the predictions of the two models"
   ],
   "metadata": {
    "id": "mmawhK9mm_Du"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Baseline model: linear regression\")\n",
    "baseline_result = baseline_evaluate(X_train, y_train, X_test, y_test)\n",
    "print_metrics(baseline_result, \"count\")\n",
    "plot_models_prediction_interval(dataset, lstm_result, baseline_result)"
   ],
   "outputs": [],
   "metadata": {
    "id": "la5JsWUpyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 (RESULT): Impact of changing students schedules on buses validations\n",
    "\n",
    "<a id='step_1'></a>\n",
    "\n",
    "Lets start with the end. We are going to answer to the question raised\n",
    "by our use case:\n",
    "\n",
    "> Could a change in the time at which students finish have a *significant*\n",
    "> impact on the number of validations in buses ?\n",
    "\n",
    "In order to answer to this question, we have trained above a machine\n",
    "learning model that we are going to use as a predictor *(please wait\n",
    "a little bit for information on the training process)*. Given a time\n",
    "(and possibly a group of students), the model outputs an estimation of\n",
    "the number of buses validations on the campus.\n",
    "\n",
    "You can play with the timeshift below and observe the impact on the \n",
    "validations. Search the following comments:\n",
    "\n",
    "```py\n",
    "####################\n",
    "# BEGIN : ...\n",
    "...\n",
    "# END : ...\n",
    "####################\n",
    "```\n"
   ],
   "metadata": {
    "id": "tawuGE-n9PTw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test predictions with classes time shift\n",
    "def shift_time(\n",
    "    dataframe: DataFrame,\n",
    "    minutes: int,\n",
    ") -> Series:\n",
    "    dataframe_ = dataframe.copy(deep=True)\n",
    "    timedelta = Timedelta(minutes, unit=\"T\")\n",
    "    dataframe_.reset_index(inplace=True)\n",
    "    dataframe_.iloc[:, [0]] += timedelta\n",
    "    dataframe_.set_index(dataframe_.columns[0], inplace=True)\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "def plot_prediction_interval_with_staggings(\n",
    "    dataframe: DataFrame,\n",
    "    staggered: DataFrame,\n",
    ") -> None:\n",
    "    figure = subplots.make_subplots(\n",
    "        rows=4,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        specs=[\n",
    "            [{\"rowspan\": 3}],\n",
    "            [None],\n",
    "            [{}],\n",
    "            [{}],\n",
    "        ],\n",
    "        vertical_spacing=0.1,\n",
    "    )\n",
    "\n",
    "    prediction_plot = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"prediction\",\n",
    "        # opacity=0.1,\n",
    "        fill=None,\n",
    "        showlegend=False,\n",
    "        # line_color=\"gray\",\n",
    "        line=dict(color=\"gray\", width=0.1),\n",
    "        # hoverinfo=\"x+y\",\n",
    "        # stackgroup='one'\n",
    "    )\n",
    "\n",
    "    figure.add_trace(prediction_plot, row=1, col=1)\n",
    "    staggered_plot = Scatter(\n",
    "        x=staggered.index,\n",
    "        y=staggered.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"staggered\",\n",
    "        # opacity=0.8,\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"red\",\n",
    "        line=dict(color=\"gray\", width=0.1),\n",
    "        # hoverinfo=\"x+y\",\n",
    "        # stackgroup='one'\n",
    "    )\n",
    "\n",
    "    figure.add_trace(staggered_plot, row=1, col=1)\n",
    "    residuals = (\n",
    "        pd.merge(\n",
    "            lstm_result,\n",
    "            staggered_lstm_result,\n",
    "            how=\"outer\",\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "        .rename(\n",
    "            {\n",
    "                \"prediction_x\": \"prediction\",\n",
    "                \"prediction_y\": \"staggered\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "        .drop([\"value_x\", \"value_y\"], axis=1)\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    residuals[\"difference\"] = residuals[\"prediction\"] - residuals[\"staggered\"]\n",
    "    colors = [\n",
    "        \"lightseagreen\" if c > 0 else \"lightsalmon\" for c in residuals[\"difference\"]\n",
    "    ]\n",
    "    bar_plot = Bar(\n",
    "        x=residuals.index,\n",
    "        y=residuals.difference,\n",
    "        name=\"difference\",\n",
    "        showlegend=False,\n",
    "        marker_color=colors,\n",
    "    )\n",
    "\n",
    "    figure.add_trace(bar_plot, row=4, col=1)\n",
    "    figure.update_xaxes(showticklabels=True, row=1, col=1)\n",
    "    figure.update_yaxes(title_text=\"difference\", row=4, col=1, zeroline=True, zerolinecolor=\"gray\")\n",
    "    figure.update_xaxes(\n",
    "        showticklabels=False,\n",
    "        visible=False,\n",
    "        row=4,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    figure.update_layout(\n",
    "        showlegend=True,\n",
    "        title_text=\"Predictions and Staggings\",\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.show()\n",
    "\n",
    "\n",
    "def evaluate_shift_time(\n",
    "    buses: DataFrame,\n",
    "    classes: DataFrame,\n",
    "    runner: RunnerHelper,\n",
    "    scaler: MinMaxScaler,\n",
    "    test_bound: Timestamp,\n",
    "    *,\n",
    "    minutes: int,\n",
    ") -> DataFrame:\n",
    "    staggered_classes = shift_time(classes, minutes=minutes)\n",
    "    dataframe = merge_datasets(staggered_classes, buses)\n",
    "    dataframe = add_features(dataframe, holidays=True)\n",
    "    test_dataset = dataframe[dataframe.index >= test_bound]\n",
    "    X_test, y_test = features_split(\n",
    "        test_dataset,\n",
    "        target=\"count\",\n",
    "    )\n",
    "\n",
    "    _, _, test_loader = to_dataloaders(\n",
    "        (X_train, y_train),\n",
    "        (X_val, y_val),\n",
    "        (X_test, y_test),\n",
    "        scaler,\n",
    "        BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    predictions, values = runner.evaluate(test_loader)\n",
    "    staggered_lstm_result = inverse_transform(\n",
    "        values,\n",
    "        predictions,\n",
    "        X_test.index,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    return staggered_lstm_result"
   ],
   "outputs": [],
   "metadata": {
    "id": "BOTmg4mEyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####################\n",
    "# BEGIN : play\n",
    "\n",
    "SHIFT_IN_MINUTES = 15\n",
    "\n",
    "# END : play  \n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# reload original buses dataset for iterative modifications\n",
    "buses_dataset = load_data(buses_path)\n",
    "buses_dataset = pre_process_by_aggregation(\n",
    "    buses_dataset,\n",
    "    stops=beaulieu,\n",
    ")\n",
    "\n",
    "staggered_lstm_result = evaluate_shift_time(\n",
    "    buses_dataset,\n",
    "    classes_dataset,\n",
    "    runner,\n",
    "    scaler,\n",
    "    end_val,\n",
    "    minutes=SHIFT_IN_MINUTES,\n",
    ")\n",
    "\n",
    "plot_prediction_interval_with_staggings(\n",
    "    lstm_result,\n",
    "    staggered_lstm_result,\n",
    ")\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ],
   "outputs": [],
   "metadata": {
    "id": "wC_KSVcDyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Observe\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {
    "id": "Qb-cG2Jb4oSj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####################\n",
    "# BEGIN : Answer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Food for thoughts\n",
    "\n",
    "> 1. How can you observe the impact of changing the schedules?\n",
    "> 2. What is the expected impact of shifting the schedules by 15min?\n",
    "> 3. Is the expected impact of a 60 mins shift bigger?\n",
    "> 4. Is there a *small* shift (e.g., less than 60 mins) that would result in a large impact?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Answer\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Now you can go to the [Step 2](#step_2).**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\n",
    " - https://colab.research.google.com/drive/1enI68fTdPI2w5KKv6jyL0Lcq9Zg3BbLx?usp=sharing"
   ],
   "metadata": {
    "id": "qDEIVmT4yTk8"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NmKSCNfpfOQ-",
    "qzLkzFP3fZm-",
    "O4teYxHrfnuh",
    "-kXTLBmBfxuL",
    "ZV4MJqgogTB0",
    "F-CFufprhULZ",
    "b5Hiwx5O7gXw",
    "tawuGE-n9PTw",
    "s4gBkGhqpaL7"
   ],
   "include_colab_link": true,
   "name": "run_nn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "d5511370cecfc9f72087461b207d0bd90f18099e89758b2a61eb3e3243f66294"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit ('venv-a2r2': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}