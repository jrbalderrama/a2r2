{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrbalderrama/a2r2/blob/main/a2r2-01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qOVUmdqyTjY"
   },
   "source": [
    "RUDI Workshop: Introduction to Privacy-Preserving Data Publishing Techniques\n",
    "=====================================================================\n",
    "\n",
    "Notebook __ONE__\n",
    "----------------\n",
    "\n",
    "Tristan ALLARD & Javier ROJAS BALDERRAMA\n",
    "\n",
    "_Univ Rennes, CNRS, INRIA_\n",
    "  \n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "\n",
    "We warmly thank François Bodin and Luc Lesoil for their support on the data and the use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 (STARTER) : \n",
    "\n",
    "This hands-on tutorial is going to introduce you to the issue of *privacy-preserving personal data publishing*. You are going to follow the implementation of a concrete use-case built from open data from the Rennes Metropole area. The main question of the use-case is to know wether a change in the students schedules at the Beaulieu campus impacts the load of the buses that go through the campus. We will answer to this question based on two datasets : the validations inside the buses that stop close to the campus (with timestamps), and the number of students that terminate a class (with timestamps). Our approach consists in training a predictor that outputs the expected number of validations along the day given the number of students terminating a class along the day. However, using raw buses validations for answering to this question may lead to privacy issues because validations can be highly identifying. After having performed some reidentification attacks, you will use a perturbed version of the buses validations dataset and observe the resulting impact on our ability to answer to the main question of the use-case. \n",
    "\n",
    "We designed this tutorial to be a step-by-step guided tour. You can follow sequentially the \"Step i\" tag inside the titles of the sections. Up to you to follow the sequence proposed or to deviate from it, but be careful when leaving the track, it's wild out there ;) \n",
    "\n",
    "We divided the full journey into three topics :\n",
    "    1. The naive version\n",
    "    2. Privacy issues\n",
    "    3. The protected version\n",
    "    \n",
    "For convenience, one notebook is dedicated to each topic and **you are currently in Notebook 1.**\n",
    "\n",
    "We also ask questions. Please take the time to think about them. Trying to answer them can also help you gain a deeper understanding. And we would love reading your answers! \n",
    "\n",
    "Ready? \n",
    "\n",
    "Really?? \n",
    "\n",
    "**You can now go to Step 1!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmKSCNfpfOQ-"
   },
   "source": [
    "\n",
    "# Step 2 (PREAMBLE) : Settings and Data\n",
    "\n",
    "Not too disappointed ? So lets now have a look at the data based on which we trained the model. \n",
    "\n",
    "1.   The datasets are downloaded \n",
    "2.   The libraries required are imported and global variables are setup\n",
    "3.   The raw data are aggregated...\n",
    "4.   ... And the results are displayed. \n",
    "5.   The datasets are prepared for the training process.\n",
    "\n",
    "Observe the buses validations dataset (Section \"Display raw data\")... Can you imagine any issue ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzLkzFP3fZm-"
   },
   "source": [
    "\n",
    " ## Data download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u_Eju0G4yTjY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-22 10:33:57 URL:https://zenodo.org/record/5509313/files/classes.parquet [17736/17736] -> \"classes.parquet\" [1]\n",
      "2021-09-22 10:33:58 URL:https://zenodo.org/record/5509268/files/buses.parquet [3594503/3594503] -> \"buses.parquet\" [1]\n"
     ]
    }
   ],
   "source": [
    "# Download the data : students counts and buses validations.\n",
    "\n",
    "!wget -nv -nc https://zenodo.org/record/5509313/files/classes.parquet\n",
    "!wget -nv -nc https://zenodo.org/record/5509268/files/buses.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4teYxHrfnuh"
   },
   "source": [
    " ## Required imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PLHjpQH6yTjY"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7feafe648de8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeatMapWithTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "from errno import ENOENT\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Tuple\n",
    "\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from folium.plugins import HeatMapWithTime\n",
    "from numpy import linalg, ndarray\n",
    "from pandas import NA, DataFrame, DatetimeIndex, Series, Timedelta, Timestamp\n",
    "from plotly import subplots\n",
    "from plotly.graph_objs import Bar, Candlestick, Figure, Scatter\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import Tensor\n",
    "from torch.nn import LSTM, Linear, Module, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82e_w9dyyTjY"
   },
   "outputs": [],
   "source": [
    "# Set rendered for image output\n",
    "from IPython import get_ipython\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    pio.renderers.default = \"colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fToRyDS0yTjY"
   },
   "outputs": [],
   "source": [
    "# Set global options and define notebook constants\n",
    "\n",
    "# project base directory\n",
    "BASE_DIRECTORY = Path(\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kXTLBmBfxuL"
   },
   "source": [
    "## Raw data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQbd6zhNR-j5"
   },
   "source": [
    "### Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUo9bi14yTk8"
   },
   "outputs": [],
   "source": [
    "# Read data from file system and plot data\n",
    "\n",
    "# load dataset from file system\n",
    "def load_data(\n",
    "    path: Path,\n",
    ") -> DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(ENOENT, os.strerror(ENOENT), path)\n",
    "\n",
    "    table = pq.read_table(path)\n",
    "    return table.to_pandas()\n",
    "\n",
    "\n",
    "# buses dataset\n",
    "buses_filename = \"buses.parquet\"\n",
    "buses_path = BASE_DIRECTORY.joinpath(buses_filename)\n",
    "buses_dataset = load_data(buses_path)\n",
    "\n",
    "\n",
    "# classes dataset\n",
    "classes_filename = \"classes.parquet\"\n",
    "classes_path = BASE_DIRECTORY.joinpath(classes_filename)\n",
    "classes_dataset = load_data(classes_path)\n",
    "\n",
    "#from google.colab import data_table\n",
    "#data_table.DataTable(buses_dataset[:20000], include_index=False, num_rows_per_page=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amAWRS8-RrXl"
   },
   "source": [
    "### Display raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYS_XqJHR3Kj"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from google.colab import data_table\n",
    "#data_table.DataTable(buses_dataset[:20000], include_index=False, num_rows_per_page=20)\n",
    "####################\n",
    "# BEGIN : Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-H-J9lkbTynD"
   },
   "outputs": [],
   "source": [
    "# END : Observe\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# BEGIN : Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thoughts: \n",
    "   1. Is there any information directly identifying in the raw data ? \n",
    "   2. Could you describe possible auxiliary information that could lead to re-identifications ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END : Answer\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can now go to Notebook 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3ZxqVvcSSgF"
   },
   "source": [
    "### Agregate raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwGBHccSyTk8"
   },
   "outputs": [],
   "source": [
    "# Post-processing input data\n",
    "\n",
    "# post processing (aggregated) transportation data\n",
    "def post_processing_by_aggregation(\n",
    "    dataframe: DataFrame,\n",
    "    *,\n",
    "    stops: Optional[Sequence[str]],\n",
    "    ignore_weekend: bool = False,\n",
    ") -> DataFrame:\n",
    "\n",
    "    dataframe_ = dataframe.copy()\n",
    "    # filter data from 'bus_stops' only\n",
    "    if stops:\n",
    "        dataframe_ = dataframe_[dataframe_[\"stop_name\"].isin(beaulieu)]\n",
    "\n",
    "    # remove weekend information\n",
    "    if ignore_weekend:\n",
    "        dataframe_ = dataframe_.set_index(\"departure_time\")\n",
    "        dataframe_ = dataframe_[dataframe_.index.dayofweek < 5]\n",
    "\n",
    "    # aggregate dataset by stop name and departure time\n",
    "    dataframe_ = (\n",
    "        dataframe_.groupby(\n",
    "            [\n",
    "                \"stop_name\",\n",
    "                \"departure_time\",\n",
    "            ]\n",
    "        )\n",
    "        .agg({\"count\": \"sum\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return dataframe_.groupby(\"departure_time\").sum()\n",
    "\n",
    "\n",
    "def plot_dataset(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    scatter = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[column],\n",
    "        mode=\"lines\",\n",
    "        name=\"values\",\n",
    "    )\n",
    "\n",
    "    figure.add_trace(scatter)\n",
    "    figure.update_layout(\n",
    "        showlegend=False,\n",
    "        title_text=column,\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(showgrid=True)\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scDTdmHFyTk8"
   },
   "outputs": [],
   "source": [
    "# target bus stops\n",
    "beaulieu = [\n",
    "    \"Les Préales\",\n",
    "    \"Tournebride\",\n",
    "    \"Beaulieu Chimie\",\n",
    "    \"Beaulieu INSA\",\n",
    "    \"Beaulieu Restau U\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "buses_dataset = post_processing_by_aggregation(\n",
    "    buses_dataset,\n",
    "    stops=beaulieu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZV4MJqgogTB0"
   },
   "source": [
    "### Display agregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX__8ghegg3F"
   },
   "source": [
    "#### Number of validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33PDn8bpyTk8"
   },
   "outputs": [],
   "source": [
    "display(buses_dataset)\n",
    "plot_dataset(buses_dataset, \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjmGFdCDgpk3"
   },
   "source": [
    "#### Number of students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP9gJJueyTk8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(classes_dataset)\n",
    "plot_dataset(classes_dataset, \"nombre_etudiant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eOCLUJEyTk8"
   },
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(\n",
    "    classes: DataFrame,\n",
    "    buses: DataFrame,\n",
    ") -> DataFrame:\n",
    "\n",
    "    # ignore dataset entries that are not available in classes timeline\n",
    "    buses_ = buses[\n",
    "        buses.index\n",
    "        <= classes.index.max()\n",
    "        + Timedelta(\n",
    "            1,\n",
    "            unit=\"day\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # merge datasets\n",
    "    dataset = pd.merge(\n",
    "        classes,\n",
    "        buses_,\n",
    "        how=\"outer\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # fill empty values\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kpiAoimhBJ6"
   },
   "source": [
    "#### Display the merged dataset (students and buses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMIuDquXyTk8"
   },
   "outputs": [],
   "source": [
    "dataset = merge_datasets(classes_dataset, buses_dataset)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-CFufprhULZ"
   },
   "source": [
    "## Data preparation for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF56rcALkfO7"
   },
   "source": [
    "### Make dates and times understandable for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXl1HRlCyTk8"
   },
   "outputs": [],
   "source": [
    "# Add features (motifs) to the dataset\n",
    "la_rentree = Timestamp(\"2021-09-06\")\n",
    "la_toussaint = Timestamp(\"2021-11-01\")\n",
    "one_week_timedelta = Timedelta(7, unit=\"day\")\n",
    "\n",
    "# bucketize attribute\n",
    "def onehot_encode(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> DataFrame:\n",
    "    dummies = pd.get_dummies(\n",
    "        dataframe[column],\n",
    "        prefix=column,\n",
    "    )\n",
    "\n",
    "    return pd.concat(\n",
    "        [dataframe, dummies],\n",
    "        axis=1,\n",
    "    ).drop(columns=[column])\n",
    "\n",
    "\n",
    "# encode (time) column as periodic wave\n",
    "def periodic_encode(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    "    period: int,\n",
    "    start_num: int = 0,\n",
    ") -> DataFrame:\n",
    "    kwargs = {\n",
    "        f\"sin_{column}\": lambda x: np.sin(\n",
    "            2 * np.pi * (dataframe[column] - start_num) / period\n",
    "        ),\n",
    "        f\"cos_{column}\": lambda x: np.cos(\n",
    "            2 * np.pi * (dataframe[column] - start_num) / period\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return dataframe.assign(**kwargs).drop(columns=[column])\n",
    "\n",
    "\n",
    "# add uniform timeindex column\n",
    "def set_time_index(\n",
    "    dataframe: DataFrame,\n",
    "    frequence: int = 15,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "\n",
    "    # add a time index using the frequency\n",
    "    dataframe_[\"time_idx\"] = dataframe_.index - dataframe_.index.min()\n",
    "    dataframe_[\"time_idx\"] = (\n",
    "        dataframe_[\"time_idx\"].astype(\"timedelta64[m]\") // frequence\n",
    "    )\n",
    "    dataframe_[\"time_idx\"] = dataframe_[\"time_idx\"].astype(\"int_\")\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "# mark dataset ranges as holidays\n",
    "def label_holidays(\n",
    "    dataframe: DataFrame,\n",
    "    start: Timestamp,\n",
    "    end: Timestamp,\n",
    "    column=\"holiday\",\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    dataframe_[column] = 0\n",
    "    dataframe_.loc[\n",
    "        (dataframe_.index >= start) & (dataframe_.index < end),\n",
    "        column,\n",
    "    ] = 1\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "# generate lags (to track interaction throughout time)\n",
    "def generate_lags(\n",
    "    dataframe: DataFrame,\n",
    "    lags: int,\n",
    "    column: str,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    for n in range(1, lags + 1):\n",
    "        dataframe_[f\"{column}_lag_{n}\"] = dataframe_[column].shift(n)\n",
    "\n",
    "    return dataframe_.fillna(0)\n",
    "\n",
    "\n",
    "# add features to the dataset\n",
    "def add_features(\n",
    "    dataframe: DataFrame,\n",
    "    bucketize_date: bool = True,\n",
    "    periodic_time: bool = True,\n",
    "    holidays: bool = False,\n",
    "    timeindex: bool = False,\n",
    "    lags: bool = False,\n",
    "    n_lags: int = 50,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    if timeindex:\n",
    "        dataframe_ = set_time_index(dataframe_)\n",
    "\n",
    "    if bucketize_date:\n",
    "        dataframe_ = dataframe_.assign(dayofweek=dataframe_.index.dayofweek)\n",
    "        # .assign(day=dataframe.index.day)\n",
    "        # .assign(month=dataset.index.month)\n",
    "        dataframe_ = onehot_encode(dataframe_, \"dayofweek\")\n",
    "        # dataset = onehot_encode(dataset, \"month\")\n",
    "\n",
    "    if periodic_time:\n",
    "        dataframe_ = dataframe_.assign(hour=dataframe_.index.hour)\n",
    "        dataframe_ = dataframe_.assign(minute=dataframe_.index.minute)\n",
    "        dataframe_ = periodic_encode(dataframe_, \"hour\", 24, 0)\n",
    "        dataframe_ = periodic_encode(dataframe_, \"minute\", 60, 0)\n",
    "\n",
    "    if holidays:\n",
    "        dataframe_ = label_holidays(\n",
    "            dataframe_,\n",
    "            la_toussaint,\n",
    "            la_toussaint + one_week_timedelta,\n",
    "        )\n",
    "\n",
    "    if lags:\n",
    "        dataframe_ = generate_lags(dataframe_, n_lags, \"count\")\n",
    "        dataframe_ = generate_lags(dataframe_, n_lags, \"nombre_etudiant\")\n",
    "\n",
    "    # dataframe.drop([\"nombre_etudiant\"], axis=1, inplace=True)\n",
    "    return dataframe_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXBXzc0YkBwt"
   },
   "source": [
    " ### Displaying the data formatted for the machine learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLSZvIr4yTk8"
   },
   "outputs": [],
   "source": [
    "dataset = add_features(dataset, holidays=True)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiD_2WATkqsE"
   },
   "source": [
    "### Split the dataset into the train, the test, and the validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mo85diSCZMB4"
   },
   "outputs": [],
   "source": [
    "def plot_timeline2(\n",
    "    dataframe: DataFrame,\n",
    "    columns: Sequence[str],\n",
    "    delimiters: Sequence[Timestamp],\n",
    "    holidays: Tuple[Timestamp, Timestamp],\n",
    ") -> None:\n",
    "    dmin = dataframe[\"nombre_etudiant\"].values.min()\n",
    "    dmax = dataframe[\"nombre_etudiant\"].values.max()\n",
    "    figure = subplots.make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    for counter, column in enumerate(columns):\n",
    "        secondary_y = False if counter % 2 == 0 else True\n",
    "        scatter = Scatter(\n",
    "            x=dataframe.index,\n",
    "            y=dataframe[column],\n",
    "            mode=\"lines\",\n",
    "            name=column,\n",
    "        )\n",
    "      \n",
    "        figure.add_trace(\n",
    "            scatter,\n",
    "            secondary_y=secondary_y,\n",
    "        )\n",
    "      \n",
    "    for delimiter in delimiters:\n",
    "        figure.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=delimiter,\n",
    "            x1=delimiter,\n",
    "            y0=dmax,\n",
    "            y1=0,\n",
    "            line=dict(\n",
    "                # color=\"Gray\",\n",
    "                width=1,                 \n",
    "                dash=\"dashdot\",\n",
    "            ),    \n",
    "        )\n",
    "\n",
    "    figure.add_shape(\n",
    "        type=\"rect\",\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        layer=\"below\",\n",
    "        fillcolor=\"LightSeaGreen\",\n",
    "        x0=holidays[0],\n",
    "        x1=holidays[1],\n",
    "        y0=dmax,\n",
    "        y1=0,\n",
    "    )\n",
    "\n",
    "    figure.add_annotation(\n",
    "        x=holidays[0],\n",
    "        y=dmax,\n",
    "        align=\"right\",\n",
    "        text=\"holidays\",\n",
    "        showarrow=False,\n",
    "        yshift=-25,\n",
    "        textangle=90,\n",
    "        xshift=10,\n",
    "    )\n",
    "                                                                                    \n",
    "    figure.add_annotation(\n",
    "        x=delimiters[0],\n",
    "        y=dmax,\n",
    "        text=\"validation\",\n",
    "        showarrow=True,\n",
    "         yshift=-15,\n",
    "    )\n",
    "\n",
    "    figure.add_annotation(\n",
    "        x=delimiters[1],\n",
    "        y=dmax,\n",
    "        text=\"test\",\n",
    "        showarrow=True,\n",
    "    )\n",
    "\n",
    "    figure.update_shapes(dict(xref=\"x\", yref=\"y\"))\n",
    "    figure.update_yaxes(\n",
    "        rangemode=\"tozero\",\n",
    "        # type=\"log\",\n",
    "        )\n",
    "\n",
    "    figure.update_xaxes(range=[dataframe.index.min(), dataframe.index.max()])\n",
    "    figure.update_yaxes(title_text=columns[0], secondary_y=False)\n",
    "    figure.update_yaxes(title_text=columns[1], secondary_y=True)            \n",
    "    figure.update_layout(\n",
    "        title_text=\"Count of Buses & Classes\",\n",
    "        template=\"simple_white\",  \n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )     \n",
    "    )\n",
    "\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FGZj9TyyTk8"
   },
   "outputs": [],
   "source": [
    "# Split the data into test, validation, and train sets\n",
    "def features_split(\n",
    "    dataframe: DataFrame,\n",
    "    target: str,\n",
    ") -> Tuple[DataFrame, DataFrame]:\n",
    "    y = dataframe[[target]]\n",
    "    X = dataframe.drop(columns=[target])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_timestamp_bound(\n",
    "    dataframe: DataFrame,\n",
    "    weeks: int,\n",
    ") -> Timestamp:\n",
    "    timedelta = Timedelta(7 * weeks - 1, unit=\"day\")\n",
    "    timestamp = dataframe.index.min() + timedelta\n",
    "    return timestamp.normalize()\n",
    "\n",
    "\n",
    "\n",
    "def plot_timeline(\n",
    "    dataframe: DataFrame,\n",
    "    columns: Sequence[str],\n",
    "    delimiters: Sequence[Timestamp],\n",
    "    holidays: Tuple[Timestamp, Timestamp],\n",
    ") -> None:\n",
    "    dmin = dataframe[\"nombre_etudiant\"].values.min()\n",
    "    dmax = dataframe[\"nombre_etudiant\"].values.max()\n",
    "    figure = Figure()\n",
    "    for column in columns:\n",
    "        scatter = Scatter(\n",
    "            x=dataframe.index,\n",
    "            y=dataframe[column],\n",
    "            mode=\"lines\",\n",
    "            name=column,\n",
    "        )\n",
    "        figure.add_trace(scatter)\n",
    "\n",
    "    for delimiter in delimiters:\n",
    "        figure.add_trace(\n",
    "            Scatter(\n",
    "                x=[delimiter, delimiter],\n",
    "                y=[dmin, dmax],\n",
    "                mode=\"lines\",\n",
    "                name=\"end\",\n",
    "                line=dict(color=\"black\", width=1, dash=\"dashdot\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    figure.add_shape(\n",
    "        line_color=\"yellow\",\n",
    "        fillcolor=\"LightSeaGreen\",\n",
    "        opacity=0.3,        \n",
    "        x0=holidays[0],\n",
    "        x1=holidays[1],\n",
    "        layer=\"below\",\n",
    "        y0=dmax,\n",
    "        y1=0,\n",
    "        xref=\"x\",\n",
    "        yref=\"y\",\n",
    "    )\n",
    "\n",
    "    figure.update_yaxes(type=\"log\", range=[0, 4])\n",
    "    figure.update_layout(\n",
    "        showlegend=False,\n",
    "        title_text=\"Count of Buses & Classes\",\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.show()\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RcO5z5qi4iq"
   },
   "source": [
    "### Displaying the train, test, validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "melyhYOiACiE"
   },
   "outputs": [],
   "source": [
    "end_train = get_timestamp_bound(dataset, weeks=9)\n",
    "end_val = get_timestamp_bound(dataset, weeks=10)\n",
    "\n",
    "# display(dataset)\n",
    "plot_timeline2(\n",
    "    dataset,\n",
    "    [\"nombre_etudiant\", \"count\"],\n",
    "    [end_train, end_val],\n",
    "    (la_toussaint, la_toussaint + one_week_timedelta),\n",
    ")\n",
    "\n",
    "train_dataset = dataset[dataset.index < end_train]\n",
    "val_dataset = dataset[(dataset.index >= end_train) & (dataset.index < end_val)]\n",
    "test_dataset = dataset[dataset.index >= end_val]\n",
    "\n",
    "X_train, y_train = features_split(train_dataset, target=\"count\")\n",
    "X_val, y_val = features_split(val_dataset, target=\"count\")\n",
    "X_test, y_test = features_split(test_dataset, target=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5Hiwx5O7gXw"
   },
   "source": [
    "# TOOL: a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxRu24uH8vQM"
   },
   "source": [
    "## Defining our neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wjqVABVyTk8"
   },
   "outputs": [],
   "source": [
    "# Define and run a RNN model\n",
    "class LSTMModel(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            layer_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(\n",
    "            self.layer_dim,\n",
    "            x.size(0),\n",
    "            self.hidden_dim,\n",
    "        ).requires_grad_()\n",
    "\n",
    "        # initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(\n",
    "            self.layer_dim,\n",
    "            x.size(0),\n",
    "            self.hidden_dim,\n",
    "        ).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        # (squeezing is equivalent to: `out = out[:, -1, :]`)\n",
    "        out = torch.squeeze(out)\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RunnerHelper:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "\n",
    "        # set model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # make predictions\n",
    "        ŷ = self.model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(ŷ, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # reset to zero gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # returns loss\n",
    "        return loss.item()\n",
    "\n",
    "    def val_step(self, X, y):\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # make prediction\n",
    "        ŷ = self.model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(ŷ, y)\n",
    "\n",
    "        # return loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, n_epochs=50):\n",
    "        model_path = f'{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_train_losses = []\n",
    "            for x_train, y_train in train_loader:\n",
    "                # x_train = x_train.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                x_train = torch.unsqueeze(x_train, 1)\n",
    "                train_loss = self.train_step(x_train, y_train)\n",
    "                batch_train_losses.append(train_loss)\n",
    "\n",
    "            training_loss = np.mean(batch_train_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    # x_val = x_val.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                    x_val = torch.unsqueeze(x_val, 1)\n",
    "                    val_loss = self.val_step(x_val, y_val)\n",
    "                    batch_val_losses.append(val_loss)\n",
    "\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 20 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch:3d}/{n_epochs}] Training loss: {training_loss:.4f}\"\n",
    "                    f\"\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                # x_test = x_test.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                x_test = torch.unsqueeze(x_test, 1)\n",
    "                self.model.eval()\n",
    "                ŷ = self.model(x_test)\n",
    "                predictions.append(ŷ.detach().numpy())\n",
    "                values.append(y_test.detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        figure = Figure()\n",
    "        tics = [*range(len(self.train_losses) + 1)]\n",
    "        value = Scatter(\n",
    "            x=tics,\n",
    "            y=self.train_losses,\n",
    "            mode=\"lines\",\n",
    "            name=\"Training\",\n",
    "            marker=dict(),\n",
    "        )\n",
    "\n",
    "        figure.add_trace(value)\n",
    "        value = Scatter(\n",
    "            x=tics,\n",
    "            y=self.val_losses,\n",
    "            mode=\"lines\",\n",
    "            name=\"Validation\",\n",
    "            marker=dict(),\n",
    "        )\n",
    "\n",
    "        figure.add_trace(value)\n",
    "        figure.update_layout(title_text=\"Losses\")\n",
    "        figure.show()\n",
    "\n",
    "\n",
    "# rescale results and align it to original time index\n",
    "def inverse_transform(\n",
    "    values: Sequence[ndarray],\n",
    "    predictions: Sequence[ndarray],\n",
    "    index: DatetimeIndex,\n",
    "    scaler: MinMaxScaler,\n",
    ") -> DataFrame:\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    dataframe = DataFrame(\n",
    "        data={\n",
    "            \"value\": vals,\n",
    "            \"prediction\": preds,\n",
    "        },\n",
    "        index=index[: len(vals)],\n",
    "    )\n",
    "\n",
    "    dataframe = dataframe.sort_index()\n",
    "    dataframe = DataFrame(\n",
    "        scaler.inverse_transform(dataframe),\n",
    "        columns=dataframe.columns,\n",
    "        index=dataframe.index,\n",
    "    )\n",
    "\n",
    "    return dataframe.astype(\"int_\")\n",
    "\n",
    "\n",
    "def print_metrics(\n",
    "    dataframe: DataFrame,\n",
    "    value: str,\n",
    "    prediction: str = \"prediction\",\n",
    ") -> None:\n",
    "    result_metrics = {\n",
    "        \"mae\": metrics.mean_absolute_error(\n",
    "            dataframe[value],\n",
    "            dataframe[prediction],\n",
    "        ),\n",
    "        \"rmse\": metrics.mean_squared_error(\n",
    "            dataframe[value],\n",
    "            dataframe[prediction],\n",
    "        )\n",
    "        ** 0.5,\n",
    "        \"r2\": metrics.r2_score(\n",
    "            dataframe[value],\n",
    "            dataframe[prediction],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    print(\"\\tMean Absolute Error:       \", result_metrics[\"mae\"])\n",
    "    print(\"\\tRoot Mean Squared Error:   \", result_metrics[\"rmse\"])\n",
    "    print(\"\\tR^2 Score:                 \", result_metrics[\"r2\"])\n",
    "    # return result_metrics\n",
    "\n",
    "\n",
    "# show residuals as kind of OHLC Charts\n",
    "def plot_residuals(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    hovertext = []\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        hovertext.append(\n",
    "            f\"{dataframe.index[i]}<br>\"\n",
    "            f\"Real: {dataframe['value'][i]}<br>\"\n",
    "            f\"Prediction: {dataframe['prediction'][i]}\"\n",
    "        )\n",
    "\n",
    "    figure = Figure(\n",
    "        data=[\n",
    "            Scatter(\n",
    "                x=dataframe.index,\n",
    "                y=dataframe[\"value\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"reference\",\n",
    "                line=dict(color=\"lightgrey\", width=0.6, dash=\"dot\"),\n",
    "                # opacity=0.6,\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            Scatter(\n",
    "                x=dataframe.index,\n",
    "                y=dataframe[\"prediction\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"prediction\",\n",
    "                line=dict(color=\"lightblue\", width=0.6, dash=\"dot\"),\n",
    "                showlegend=False,\n",
    "                # opacity=0.6,\n",
    "            ),\n",
    "            Candlestick(\n",
    "                x=dataframe.index,\n",
    "                open=dataframe[\"value\"],\n",
    "                high=dataframe[\"prediction\"],\n",
    "                low=dataframe[\"prediction\"],\n",
    "                close=dataframe[\"value\"],\n",
    "                text=hovertext,\n",
    "                hoverinfo=\"text\",\n",
    "                name=\"residuals\",\n",
    "                # line=dict(width=2),\n",
    "                increasing_line_color=\"lightseagreen\",\n",
    "                decreasing_line_color=\"lightsalmon\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    figure.update_layout(\n",
    "        title=\"Prediction residuals\",\n",
    "        template=\"simple_white\",\n",
    "        xaxis_rangeslider_visible=True,\n",
    "    )\n",
    "\n",
    "    figure.show()\n",
    "\n",
    "\n",
    "# formating data for NN\n",
    "def to_dataloaders(\n",
    "    dataframe_train: Tuple[DataFrame, DataFrame],\n",
    "    dataframe_val: Tuple[DataFrame, DataFrame],\n",
    "    dataframe_test: Tuple[DataFrame, DataFrame],\n",
    "    scaler: MinMaxScaler,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    # scale data\n",
    "    X_train_arr = scaler.fit_transform(dataframe_train[0])\n",
    "    X_val_arr = scaler.transform(dataframe_val[0])\n",
    "    X_test_arr = scaler.transform(dataframe_test[0])\n",
    "\n",
    "    y_train_arr = scaler.fit_transform(dataframe_train[1])\n",
    "    y_val_arr = scaler.transform(dataframe_val[1])\n",
    "    y_test_arr = scaler.transform(dataframe_test[1])\n",
    "\n",
    "    # transform scaled data to tensors\n",
    "    train_features = Tensor(X_train_arr)\n",
    "    train_targets = Tensor(y_train_arr)\n",
    "    val_features = Tensor(X_val_arr)\n",
    "    val_targets = Tensor(y_val_arr)\n",
    "    test_features = Tensor(X_test_arr)\n",
    "    test_targets = Tensor(y_test_arr)\n",
    "\n",
    "    # setup tensor datasets\n",
    "    train = TensorDataset(train_features, train_targets)\n",
    "    val = TensorDataset(val_features, val_targets)\n",
    "    test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    # setup (tensor) datasets loaders\n",
    "    train_loader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test,\n",
    "        batch_size=1,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX7rcuEslD4T"
   },
   "source": [
    "## Parameterizing our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mgESKb_yTk8"
   },
   "outputs": [],
   "source": [
    "# NN parameters\n",
    "HIDDEN_DIM = 64\n",
    "LAYER_DIM = 3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UORhj0JelLX3"
   },
   "source": [
    "## Training our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr5aNd4VyTk8"
   },
   "outputs": [],
   "source": [
    "input_dim = len(X_train.columns)  # X_train.shape[0]\n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    layer_dim=LAYER_DIM,\n",
    "    output_dim=1,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler()  # RobustScaler()  # StandardScaler()  # MinMaxScaler()\n",
    "loss_fn = MSELoss()  # L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "runner = RunnerHelper(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "train_loader, val_loader, test_loader = to_dataloaders(\n",
    "    (X_train, y_train),\n",
    "    (X_val, y_val),\n",
    "    (X_test, y_test),    \n",
    "    scaler,\n",
    "    BATCH_SIZE,\n",
    ")\n",
    "\n",
    "runner.train(train_loader, val_loader, n_epochs=EPOCHS)\n",
    "runner.plot_losses()\n",
    "predictions, values = runner.evaluate(test_loader)\n",
    "lstm_result = inverse_transform(values, predictions, X_test.index, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmxvTcwSlaqm"
   },
   "source": [
    "## Visualising the quality of our neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Th2MSHU5lrfL"
   },
   "outputs": [],
   "source": [
    "print(f\"NN model: LSTM\")\n",
    "print_metrics(lstm_result, \"value\")\n",
    "display(lstm_result)\n",
    "plot_residuals(lstm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaQUkM6X8_2s"
   },
   "source": [
    "## Comparing our neural network against a baseline method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8nCaMtem6sC"
   },
   "source": [
    " ### Train a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNEy9gZNyTk8"
   },
   "outputs": [],
   "source": [
    "# Build a baseline model to compare against the RNN model\n",
    "def baseline_evaluate(\n",
    "    X_train: DataFrame,\n",
    "    y_train: DataFrame,\n",
    "    X_test: DataFrame,\n",
    "    y_test: DataFrame,\n",
    ") -> DataFrame:\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    dataframe = DataFrame(y_test)\n",
    "    dataframe = dataframe.assign(prediction=prediction)\n",
    "    dataframe = dataframe.sort_index()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def plot_models_prediction_interval(\n",
    "    dataframe: DataFrame,\n",
    "    rnn_dataframe: DataFrame,\n",
    "    baseline_dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    value = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[\"count\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Reference\",\n",
    "        line=dict(color=\"rgba(0,0,0, 0.3)\", width=1, dash=\"dot\"),\n",
    "    )\n",
    "\n",
    "    figure.add_trace(value)\n",
    "    baseline = Scatter(\n",
    "        x=baseline_dataframe.index,\n",
    "        y=baseline_dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"Linear Regression\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    "\n",
    "    figure.add_trace(baseline)\n",
    "    prediction = Scatter(\n",
    "        x=rnn_dataframe.index,\n",
    "        y=rnn_dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"LSTM NN\",\n",
    "        # marker=dict(),\n",
    "        opacity=0.8,\n",
    "        visible=\"legendonly\",\n",
    "    )\n",
    "\n",
    "    figure.add_trace(prediction)\n",
    "    figure.update_layout(\n",
    "        showlegend=True,\n",
    "        title_text=\"Predictions\",\n",
    "        template=\"simple_white\",\n",
    "        xaxis=dict(\n",
    "            range=[\n",
    "                rnn_dataframe.index.min(),\n",
    "                rnn_dataframe.index.max(),\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(rangeslider_visible=True)\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmawhK9mm_Du"
   },
   "source": [
    "### Visualize the predictions of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "la5JsWUpyTk8"
   },
   "outputs": [],
   "source": [
    "print(\"Baseline model: linear regression\")\n",
    "baseline_result = baseline_evaluate(X_train, y_train, X_test, y_test)\n",
    "print_metrics(baseline_result, \"count\")\n",
    "plot_models_prediction_interval(dataset, lstm_result, baseline_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tawuGE-n9PTw"
   },
   "source": [
    "# Step 1 (RESULT) : impact of changing the students schedules on the buses validations\n",
    "\n",
    "Lets start with the end. We are going to answer to the question raised by our use case : \n",
    "> **Could a change in the time at which students \n",
    "finish have a *significant* impact on the number of validations in buses ?**\n",
    "\n",
    "In order to answer to this question, we have trained above a machine learning model that we are going to use as a predictor *(please wait a little bit for information on the training process)*. Given a time (and possibly a group of students), the model outputs an estimation of the number of buses validations on the campus. \n",
    "\n",
    "You can play with the timeshift below and observe the impact on the validations. Search the following comments : \n",
    "```\n",
    "####################\n",
    "# BEGIN : ...\n",
    "...\n",
    "# END : ...\n",
    "####################\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOTmg4mEyTk8"
   },
   "outputs": [],
   "source": [
    "# Test predictions with classes time shift\n",
    "def shift_time(\n",
    "    dataframe: DataFrame,\n",
    "    minutes: int,\n",
    ") -> Series:\n",
    "    dataframe_ = dataframe.copy(deep=True)\n",
    "    timedelta = Timedelta(minutes, unit=\"T\")\n",
    "    dataframe_.reset_index(inplace=True)\n",
    "    dataframe_.iloc[:, [0]] += timedelta\n",
    "    dataframe_.set_index(dataframe_.columns[0], inplace=True)\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "def plot_prediction_interval_with_staggings(\n",
    "    dataframe: DataFrame,\n",
    "    staggered: DataFrame,\n",
    ") -> None:\n",
    "    figure = subplots.make_subplots(\n",
    "        rows=4,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        specs=[\n",
    "            [{\"rowspan\": 3}],\n",
    "            [None],\n",
    "            [{}],\n",
    "            [{}],\n",
    "        ],\n",
    "        vertical_spacing=0.1,\n",
    "    )\n",
    "\n",
    "    prediction_plot = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"prediction\",\n",
    "        # opacity=0.1,\n",
    "        fill=None,\n",
    "        showlegend=False,\n",
    "        # line_color=\"gray\",\n",
    "        line=dict(color=\"gray\", width=0.1),\n",
    "        # hoverinfo=\"x+y\",\n",
    "        # stackgroup='one'\n",
    "    )\n",
    "\n",
    "    figure.add_trace(prediction_plot, row=1, col=1)\n",
    "    staggered_plot = Scatter(\n",
    "        x=staggered.index,\n",
    "        y=staggered.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"staggered\",\n",
    "        # opacity=0.8,\n",
    "        fill=\"tonexty\",\n",
    "        fillcolor=\"red\",\n",
    "        line=dict(color=\"gray\", width=0.1),\n",
    "        # hoverinfo=\"x+y\",\n",
    "        # stackgroup='one'\n",
    "    )\n",
    "\n",
    "    figure.add_trace(staggered_plot, row=1, col=1)\n",
    "    residuals = (\n",
    "        pd.merge(\n",
    "            lstm_result,\n",
    "            staggered_lstm_result,\n",
    "            how=\"outer\",\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "        .rename(\n",
    "            {\n",
    "                \"prediction_x\": \"prediction\",\n",
    "                \"prediction_y\": \"staggered\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "        .drop([\"value_x\", \"value_y\"], axis=1)\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    residuals[\"difference\"] = residuals[\"prediction\"] - residuals[\"staggered\"]\n",
    "    colors = [\n",
    "        \"lightseagreen\" if c > 0 else \"lightsalmon\" for c in residuals[\"difference\"]\n",
    "    ]\n",
    "    bar_plot = Bar(\n",
    "        x=residuals.index,\n",
    "        y=residuals.difference,\n",
    "        name=\"difference\",\n",
    "        showlegend=False,\n",
    "        marker_color=colors,\n",
    "    )\n",
    "\n",
    "    figure.add_trace(bar_plot, row=4, col=1)\n",
    "    figure.update_xaxes(showticklabels=True, row=1, col=1)\n",
    "    figure.update_yaxes(title_text=\"difference\", row=4, col=1, zeroline=True, zerolinecolor=\"gray\")\n",
    "    figure.update_xaxes(\n",
    "        showticklabels=False,\n",
    "        visible=False,\n",
    "        row=4,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    figure.update_layout(\n",
    "        showlegend=True,\n",
    "        title_text=\"Predictions and Staggings\",\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.show()\n",
    "\n",
    "\n",
    "def evaluate_shift_time(\n",
    "    buses: DataFrame,\n",
    "    classes: DataFrame,\n",
    "    runner: RunnerHelper,\n",
    "    scaler: MinMaxScaler,\n",
    "    test_bound: Timestamp,\n",
    "    *,\n",
    "    minutes: int,\n",
    ") -> DataFrame:\n",
    "    staggered_classes = shift_time(classes, minutes=minutes)\n",
    "    dataframe = merge_datasets(staggered_classes, buses)\n",
    "    dataframe = add_features(dataframe, holidays=True)\n",
    "    test_dataset = dataframe[dataframe.index >= test_bound]\n",
    "    X_test, y_test = features_split(\n",
    "        test_dataset,\n",
    "        target=\"count\",\n",
    "    )\n",
    "\n",
    "    _, _, test_loader = to_dataloaders(\n",
    "        (X_train, y_train),\n",
    "        (X_val, y_val),\n",
    "        (X_test, y_test),\n",
    "        scaler,\n",
    "        BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    predictions, values = runner.evaluate(test_loader)\n",
    "    staggered_lstm_result = inverse_transform(\n",
    "        values,\n",
    "        predictions,\n",
    "        X_test.index,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "    return staggered_lstm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbe2Nm7fyTk8"
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# BEGIN : play\n",
    "\n",
    "SHIFT_IN_MINUTES = 95\n",
    "\n",
    "# END : play  \n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC_KSVcDyTk8"
   },
   "outputs": [],
   "source": [
    "# reload original buses dataset for iterative modifications\n",
    "buses_dataset = load_data(buses_path)\n",
    "buses_dataset = post_processing_by_aggregation(\n",
    "    buses_dataset,\n",
    "    stops=beaulieu,\n",
    ")\n",
    "\n",
    "staggered_lstm_result = evaluate_shift_time(\n",
    "    buses_dataset,\n",
    "    classes_dataset,\n",
    "    runner,\n",
    "    scaler,\n",
    "    end_val,\n",
    "    minutes=SHIFT_IN_MINUTES,\n",
    ")\n",
    "\n",
    "plot_prediction_interval_with_staggings(\n",
    "    lstm_result,\n",
    "    staggered_lstm_result,\n",
    ")\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qb-cG2Jb4oSj"
   },
   "outputs": [],
   "source": [
    "# END : Observe\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# BEGIN : Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thoughts: \n",
    "    1. How can you observe the impact of changing the schedules ? \n",
    "    2. What is the expected impact of shifting the schedules by 15mins ? \n",
    "    3. Is the expected impact of a 60 mins shift bigger ?\n",
    "    4. Is there a _small_ shift (e.g., less than 60 mins) that would result in a large impact ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END : Answer\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can now go to Step 2 (same notebook).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD991wF88Rio"
   },
   "source": [
    "# Step 3 (ATTACK): the case for privacy\n",
    "\n",
    "**(Switch to Notebook 2 please)**\n",
    "\n",
    "Yes, raw data is not immune to reidentification ! \n",
    "\n",
    "You are now going to perform a reidentification attack on a small set of targets. To this end, we will give you some auxiliary information (also called background knowledge) and programming tools for helping you query the dataset. \n",
    "\n",
    "But first lets visualize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4gBkGhqpaL7"
   },
   "source": [
    "## Displaying raw buses validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-1jePEeyTk8"
   },
   "outputs": [],
   "source": [
    "# Privacy metrics\n",
    "\n",
    "# show detailed dataset\n",
    "def display_heatmap_with_time(\n",
    "    dataframe: DataFrame,\n",
    "    group_column: str = \"departure_time\",\n",
    "    # Rennes GPS coordinates\n",
    "    location: Tuple[float, float] = (48.1147, -1.6794),\n",
    ") -> None:\n",
    "    _dataframe = dataframe.copy(deep=True)\n",
    "    timestamps = []\n",
    "    coordinates = []\n",
    "    for timestamp, coordinate in _dataframe.groupby(group_column):\n",
    "        timestamps.append(str(timestamp))\n",
    "        coordinates.append(\n",
    "            coordinate[\n",
    "                [\n",
    "                    \"stop_lat\",\n",
    "                    \"stop_lon\",\n",
    "                ]\n",
    "            ].values.tolist()\n",
    "        )\n",
    "\n",
    "    base_map = folium.Map(\n",
    "        location=location,\n",
    "        zoom_start=11,\n",
    "        tiles=\"https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png\",\n",
    "        # tiles=\"https://{s}.basemaps.cartocdn.com/dark_nolabels/{z}/{x}/{y}{r}.png\",\n",
    "        attr=\"CartoDB\",\n",
    "    )\n",
    "\n",
    "    heat_map = HeatMapWithTime(\n",
    "        data=coordinates,\n",
    "        index=timestamps,\n",
    "        auto_play=True,\n",
    "        min_speed=1,\n",
    "        radius=4,\n",
    "        max_opacity=0.5,\n",
    "    )\n",
    "\n",
    "    heat_map.add_to(base_map)\n",
    "    display(base_map)\n",
    "\n",
    "\n",
    "def get_metrics_dataset(\n",
    "    path: Path,\n",
    ") -> DataFrame:\n",
    "    dataframe = load_data(path)\n",
    "    \n",
    "    # drop geospatial attributes from dataset\n",
    "    return dataframe[\n",
    "        [\n",
    "            \"departure_time\",\n",
    "            \"id\",\n",
    "            \"stop_name\",\n",
    "            \"route_short_name\",\n",
    "            \"stop_id\",\n",
    "            \"direction_id\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwNpJzamyTk8"
   },
   "outputs": [],
   "source": [
    "buses_dataset = load_data(buses_path)\n",
    "display_heatmap_with_time(buses_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65WnAyybppNH"
   },
   "source": [
    "## Attacking raw buses validations\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgTWdPnapwFd"
   },
   "source": [
    "## Explaining the success of the attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OHcTzRI9oQo"
   },
   "source": [
    "### Shannon's entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AS66DfaDyTk8"
   },
   "outputs": [],
   "source": [
    "# Shannon's entropy\n",
    "def entropy(\n",
    "    series: Series,\n",
    "    base: int = 2,\n",
    "    normalize: bool = False,\n",
    ") -> float:\n",
    "    def expectation(probability: Series) -> float:\n",
    "        return (probability * np.log(probability) / np.log(base)).sum()\n",
    "\n",
    "    def efficiency(entropy: float, length: int) -> float:\n",
    "        return entropy * np.log(base) / np.log(length)\n",
    "\n",
    "    probability = series.value_counts(normalize=True, sort=False)\n",
    "    h = -expectation(probability)\n",
    "    return efficiency(h, series.size) if normalize else h\n",
    "\n",
    "\n",
    "def get_entropies(\n",
    "    dataframe: DataFrame,\n",
    "    base: int = 2,\n",
    "    normalize: bool = False,\n",
    ") -> Series:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    entropies = dataframe_.apply(\n",
    "        entropy,\n",
    "        base=base,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        entropies.to_frame()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\n",
    "                \"index\": \"attribute\",\n",
    "                0: \"entropy\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_entropies(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = px.bar(\n",
    "        dataframe,\n",
    "        x=\"entropy\",\n",
    "        y=\"attribute\",\n",
    "        orientation=\"h\",\n",
    "        color=\"attribute\",\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "\n",
    "    figure.update_traces(\n",
    "        texttemplate=\"%{x:.2f}\",\n",
    "        textposition=\"auto\",\n",
    "    )\n",
    "\n",
    "    figure.update_layout(showlegend=False)\n",
    "\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckEdKVZZyTk8"
   },
   "outputs": [],
   "source": [
    "dataset = get_metrics_dataset(buses_path)\n",
    "display(dataset)\n",
    "entropies = get_entropies(dataset, normalize=True)\n",
    "plot_entropies(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0bECEV09vVw"
   },
   "source": [
    "### Anonymity Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKBT8XzLyTk8"
   },
   "outputs": [],
   "source": [
    "# Anonymity set\n",
    "def get_anonymity_set(\n",
    "    dataframe: DataFrame,\n",
    "    subset: Optional[Sequence[str]] = None,\n",
    "    reindex: bool = False,\n",
    ") -> Series:\n",
    "    def reset_index(serie: Series) -> Series:\n",
    "        domain = range(1, serie.index.max() + 1)\n",
    "        return serie.reindex(domain, fill_value=0)\n",
    "\n",
    "    dataframe_ = dataframe.copy()\n",
    "    multiplicity = dataframe_.value_counts(subset=subset)\n",
    "    aset = multiplicity.value_counts().sort_index()\n",
    "    aset = reset_index(aset) if reindex else aset\n",
    "    return (\n",
    "        aset.to_frame()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\n",
    "                \"index\": \"cardinality\",\n",
    "                0: \"occurrences\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_anonymity_set(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = px.bar(\n",
    "        dataframe,\n",
    "        x=\"cardinality\",\n",
    "        y=\"occurrences\",\n",
    "        color=\"occurrences\",\n",
    "        color_continuous_scale=\"Bluered\",\n",
    "        # template=\"plotly_white\",\n",
    "        title=\"Anonymity Set\",\n",
    "    )\n",
    "\n",
    "    figure.update_coloraxes(showscale=False)\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pE25rm9iyTk8"
   },
   "outputs": [],
   "source": [
    "dataset = get_metrics_dataset(buses_path)\n",
    "anonymity_set = get_anonymity_set(\n",
    "    dataset,\n",
    "    subset=[\n",
    "        \"id\",\n",
    "        \"stop_name\",\n",
    "        \"route_short_name\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "plot_anonymity_set(anonymity_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB4i1qf492Js"
   },
   "source": [
    "# SOLUTION: Sound protection with differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld5BGBJEyTk8"
   },
   "outputs": [],
   "source": [
    "# Perturb timeline series with differential privacy\n",
    "def fpa(Q: ndarray, δ: float, ε: float, k: int) -> ndarray:\n",
    "\n",
    "    # define a laplace mechanism for perturbation\n",
    "    def lpa(Q: ndarray, δ: float, ε: float) -> ndarray:\n",
    "        # differential privacy scale based on the budget\n",
    "        λ = δ / ε\n",
    "\n",
    "        # Laplace mechanism applied to whole serie\n",
    "        Z = np.random.laplace(scale=λ, size=Q.size)\n",
    "\n",
    "        return Q + Z\n",
    "\n",
    "    # discrete Fourier trasform\n",
    "    F = np.fft.fft(Q)\n",
    "\n",
    "    # first k values of DFT\n",
    "    F_k = F[:k]\n",
    "\n",
    "    # lpa of F_k\n",
    "    Fλ_k = lpa(F_k.real, δ, ε) + 1j * lpa(F_k.imag, δ, ε)\n",
    "\n",
    "    # Fλ_k with `n - k` zero-padding\n",
    "    Fλ_n = np.pad(Fλ_k, (0, Q.size - k))\n",
    "\n",
    "    # inverse discrete Fourier transform\n",
    "    Qλ = np.fft.ifft(Fλ_n)\n",
    "\n",
    "    # modulus of complex values of IFFT\n",
    "    Qλ_m = np.absolute(Qλ)\n",
    "\n",
    "    # round perturbation to integers\n",
    "    Qλ_int = np.rint(Qλ_m)\n",
    "\n",
    "    # replace negative values with zeroes\n",
    "    Qλ_int[Qλ_int < 0] = 0\n",
    "\n",
    "    return Qλ_int\n",
    "\n",
    "\n",
    "# perform a noise perturbation with the Rastogi algorithm\n",
    "def fourier_perturbation(\n",
    "    sequence: Series,\n",
    "    boundary: float,\n",
    "    budget: float,\n",
    "    coefficients: int,\n",
    ") -> Optional[ndarray]:\n",
    "\n",
    "    # calculate the L-norm of a uniform vector of seed values\n",
    "    def norm(seed: float, size: int, order: int) -> float:\n",
    "        serie = np.full((size,), seed)\n",
    "        return linalg.norm(serie, order)\n",
    "\n",
    "    size = sequence.size\n",
    "    if size  > coefficients:\n",
    "        sensitivity = math.sqrt(coefficients) * norm(boundary, size, 2)\n",
    "        return fpa(\n",
    "            sequence.to_numpy(),\n",
    "            sensitivity,\n",
    "            budget,\n",
    "            coefficients,\n",
    "        )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def bound(\n",
    "    serie: Series,\n",
    "    aggregate: str,\n",
    ") -> float:\n",
    "    def ceil(serie: Series) -> float:\n",
    "        maximum = serie.max()\n",
    "        # maximum = linalg.norm(Q, np.inf)\n",
    "        # # round(maximum, -1)\n",
    "        return 10 * math.ceil(maximum / 10)\n",
    "\n",
    "    return {\n",
    "        \"count\": 1,\n",
    "        \"sum\": ceil(serie),\n",
    "    }.get(aggregate, NA)\n",
    "\n",
    "\n",
    "def facet_plot(\n",
    "    dataframe: DataFrame,\n",
    "    size: int,\n",
    "    row: str,\n",
    "    col: str,\n",
    ") -> None:\n",
    "    dataset = dataframe.query(f\"n=={size}\").reset_index()\n",
    "    figure = px.line(\n",
    "        dataset,\n",
    "        x=\"departure_time\",\n",
    "        y=\"fpa\",\n",
    "        facet_row=row,\n",
    "        facet_col=col,\n",
    "        labels = {'departure_time': '', 'fpa': ''},\n",
    "        #facet_row_spacing=0.01,\n",
    "        #facet_col_spacing=0.01,\n",
    "    )\n",
    "                                                                                                                                    \n",
    "    figure.update_yaxes(matches=None, showticklabels=False)\n",
    "    figure.update_xaxes(showticklabels=False)\n",
    "    #figure.update_coloraxes(showscale=False)\n",
    "                                                                                                                                                    \n",
    "    trace = Scatter(\n",
    "        x=dataset.departure_time, \n",
    "        y=dataset.validation,\n",
    "        name=\"count\", \n",
    "        line=dict(color=\"gray\", width=0.1, dash=\"dot\"),  \n",
    "        opacity=0.35,\n",
    "    )\n",
    "\n",
    "    trace.update(showlegend=False)\n",
    "    for i, _ in enumerate(dataset[row].unique(), start=1):\n",
    "        for j, _ in enumerate(dataset[col].unique(), start=1):\n",
    "            figure.add_trace(trace, row=i, col=j)\n",
    "\n",
    "    figure.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title=f\"FPA for n={size}\",\n",
    "        xaxis_title=\"date\",\n",
    "        yaxis_title=\"count\"\n",
    "    )\n",
    "\n",
    "    figure.show()\n",
    "\n",
    "def get_fourier_perturbations(\n",
    "    dataframe: DataFrame,\n",
    "    agg_sizes: Sequence[int],\n",
    "    coefficients: Sequence[int],\n",
    "    epsilons: Sequence[float],\n",
    ") -> DataFrame:\n",
    "    # count validations by bus stop (per user and timestamp)\n",
    "    dataframe_ = (\n",
    "        dataframe.groupby([\"id\", \"departure_time\"])\n",
    "        .count()[\"stop_id\"]\n",
    "        .to_frame()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\"stop_id\": \"validation\"},\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    samples = DataFrame()\n",
    "    for n in agg_sizes:\n",
    "        subset = dataframe_[\"id\"].drop_duplicates().sample(n).values\n",
    "        mask = dataframe_[\"id\"].isin(subset)\n",
    "        sample = dataframe_[mask].reset_index(drop=True)\n",
    "        sample = sample.assign(n=n).drop(\"id\", axis=1)\n",
    "        samples = samples.append(sample)\n",
    "\n",
    "    fpas = DataFrame()\n",
    "    for n in agg_sizes:\n",
    "        sample = samples.query(f\"n=={n}\")\n",
    "        reference = sample.groupby(\"departure_time\").aggregate(\"count\")\n",
    "        boundary = bound(sample[\"validation\"], \"count\")\n",
    "        for k, ε in itertools.product(coefficients, epsilons):\n",
    "            iteration = reference.copy()\n",
    "            iteration = iteration.assign(n=n, ε=ε, k=k)\n",
    "            iteration[\"fpa\"] = fourier_perturbation(\n",
    "                iteration[\"validation\"],\n",
    "                boundary,\n",
    "                ε,\n",
    "                k,\n",
    "            )\n",
    "\n",
    "            iteration[\"noise\"] = iteration[\"fpa\"] - iteration[\"validation\"]\n",
    "            fpas = fpas.append(iteration)\n",
    "\n",
    "    return fpas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSqlIMHjyTk8"
   },
   "outputs": [],
   "source": [
    "dataset = get_metrics_dataset(buses_path)\n",
    "\n",
    "# aggregate size\n",
    "Ν = [50, 100, 500, 1000, 3500]\n",
    "\n",
    "# Fourier coefficients\n",
    "Κ = [10, 20, 30, 40] # , 50]\n",
    "\n",
    "# perturbation budget\n",
    "Ε = [0.01, 0.1] #,1.0, 10.0]\n",
    "\n",
    "fpas = get_fourier_perturbations(dataset, Ν, Κ, Ε)\n",
    "facet_plot(fpas, 3500, row=\"ε\", col=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beOhay5kqCw9"
   },
   "source": [
    "## Training a safe neural network\n",
    "TODO\n",
    "\n",
    "## Comparing the results\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDEIVmT4yTk8"
   },
   "source": [
    " Some references:\n",
    " - https://colab.research.google.com/drive/1enI68fTdPI2w5KKv6jyL0Lcq9Zg3BbLx?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NmKSCNfpfOQ-",
    "qzLkzFP3fZm-",
    "O4teYxHrfnuh",
    "-kXTLBmBfxuL",
    "ZV4MJqgogTB0",
    "F-CFufprhULZ",
    "b5Hiwx5O7gXw",
    "tawuGE-n9PTw",
    "s4gBkGhqpaL7"
   ],
   "include_colab_link": true,
   "name": "run_nn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "d5511370cecfc9f72087461b207d0bd90f18099e89758b2a61eb3e3243f66294"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
