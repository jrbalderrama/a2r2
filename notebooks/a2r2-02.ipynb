{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrbalderrama/a2r2/blob/main/notebooks/a2r2-02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUDI Workshop: Introduction to Privacy-Preserving Data Publishing Techniques\n",
    "\n",
    "Tristan ALLARD & Javier ROJAS BALDERRAMA\n",
    "\n",
    "_Univ Rennes, CNRS, INRIA_\n",
    "  \n",
    "This work is licensed under a [Creative Commons Zero v1.0 Universal License](https://creativecommons.org/publicdomain/zero/1.0/)"
   ],
   "metadata": {
    "id": "5qOVUmdqyTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook __TWO__: The case for privacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preamble\n",
    "\n",
    "Yes, raw data is not immune to re-identification! \n",
    "\n",
    "You are now going to perform a reidentification attack on a small set of targets. To this end, we will give you some auxiliary information (also called background knowledge) and programming tools for helping you query the dataset.\n",
    "1. You can display the buses validations dataset [here](#displayvalid). Feel free to to play with the filter menu,although the number of shown rows is limited. \n",
    "2. You can attack the dataset [Step 1](#attack) (do not be afraid to try!). \n",
    "3. In order to understand better your attacks and/or design other attacks, you can display informative measures about the _identifying power_ of the attributes of the dataset ([Step 2](#explain)). "
   ],
   "metadata": {
    "id": "OD991wF88Rio"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    " ### Download dataset\n"
   ],
   "metadata": {
    "id": "qzLkzFP3fZm-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -nv -nc https://zenodo.org/record/5509268/files/buses.parquet"
   ],
   "outputs": [],
   "metadata": {
    "id": "u_Eju0G4yTjY",
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import required modules"
   ],
   "metadata": {
    "id": "O4teYxHrfnuh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import copy\n",
    "import importlib\n",
    "import os\n",
    "from errno import ENOENT\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pyarrow.parquet as pq\n",
    "from folium.plugins import HeatMapWithTime\n",
    "from IPython import display, get_ipython\n",
    "from pandas import NA, DataFrame, Series, Timestamp\n",
    "from plotly.graph_objs import Figure, Scatter"
   ],
   "outputs": [],
   "metadata": {
    "id": "PLHjpQH6yTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup notebook constants and running environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# project base directory\n",
    "BASE_DIRECTORY = Path(\".\")\n",
    "\n",
    "# detect running environment\n",
    "COLAB_ON = True if \"google.colab\" in str(get_ipython()) else False"
   ],
   "outputs": [],
   "metadata": {
    "id": "fToRyDS0yTjY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set Ploty renderer\n",
    "if COLAB_ON:\n",
    "    pio.renderers.default = \"colab\""
   ],
   "outputs": [],
   "metadata": {
    "id": "82e_w9dyyTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and display raw dataset"
   ],
   "metadata": {
    "id": "s4gBkGhqpaL7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load dataset from file system\n",
    "def load_data(\n",
    "    path: Path,\n",
    ") -> DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(ENOENT, os.strerror(ENOENT), path)\n",
    "\n",
    "    table = pq.read_table(path)\n",
    "    return table.to_pandas()\n",
    "\n",
    "\n",
    "# show a dataframe as a table\n",
    "def display_dataframe(\n",
    "        dataframe: DataFrame,\n",
    ") -> None:    \n",
    "    if COLAB_ON:\n",
    "        spec = importlib.util.find_spec(\"google.colab\")\n",
    "        if spec:            \n",
    "            data_table = importlib.import_module(\"google.colab.data_table\")            \n",
    "            enable_dataframe_formatter = getattr(\n",
    "                data_table, \n",
    "                \"enable_dataframe_formatter\",\n",
    "            )            \n",
    "            \n",
    "            enable_dataframe_formatter()            \n",
    "           \n",
    "    display.display(dataframe[:20000] if COLAB_ON else dataframe) "
   ],
   "outputs": [],
   "metadata": {
    "id": "iUo9bi14yTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Show raw dataset\n",
    "\n",
    "<a id=\"displayvalid\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = BASE_DIRECTORY.joinpath(\"buses.parquet\")\n",
    "buses_dataset = load_data(path)\n",
    "display_dataframe(buses_dataset)\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show dataset on a map\n",
    "def plot_heatmap(\n",
    "    dataframe: DataFrame,\n",
    "    group_column: str = \"departure_time\",\n",
    "    # Rennes GPS coordinates\n",
    "    location: Tuple[float, float] = (48.1147, -1.6794),\n",
    ") -> None:\n",
    "    _dataframe = dataframe.copy(deep=True)\n",
    "    timestamps = []\n",
    "    coordinates = []\n",
    "    for timestamp, coordinate in _dataframe.groupby(group_column):\n",
    "        timestamps.append(str(timestamp))\n",
    "        coordinates.append(\n",
    "            coordinate[\n",
    "                [\n",
    "                    \"stop_lat\",\n",
    "                    \"stop_lon\",\n",
    "                ]\n",
    "            ].values.tolist()\n",
    "        )\n",
    "\n",
    "    base_map = folium.Map(\n",
    "        location=location,\n",
    "        zoom_start=11,\n",
    "        tiles=\"https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png\",\n",
    "        # tiles=\"https://{s}.basemaps.cartocdn.com/dark_nolabels/{z}/{x}/{y}{r}.png\",\n",
    "        attr=\"CartoDB\",\n",
    "    )\n",
    "\n",
    "    heat_map = HeatMapWithTime(\n",
    "        data=coordinates,\n",
    "        index=timestamps,\n",
    "        auto_play=True,\n",
    "        min_speed=1,\n",
    "        radius=4,\n",
    "        max_opacity=0.5,\n",
    "    )\n",
    "\n",
    "    heat_map.add_to(base_map)\n",
    "    display.display(base_map)"
   ],
   "outputs": [],
   "metadata": {
    "id": "p-1jePEeyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Showing the heat map of validations only works on a local server\n",
    "if not COLAB_ON:\n",
    "    plot_heatmap(buses_dataset)"
   ],
   "outputs": [],
   "metadata": {
    "id": "KwNpJzamyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Observe\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Attack raw buses validations\n",
    "<a id=\"attack\"></a>\n",
    "\n",
    "Re-identification attacks are simple conceptually. They consist in selecting the subset of individuals whose records match the auxiliary information that the attacker has about them. If a single individual matches the adversarial knowledge, the success of the attack is clear (assuming that the adversarial knowledge is reliable). But when more than a single individual match the adversarial knowledge, is it a failure? \n",
    "\n",
    "By \"looking\" at the dataset (see above), could you imagine stronger auxiliary information?"
   ],
   "metadata": {
    "id": "65WnAyybppNH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# drop geospatial attributes from dataset\n",
    "def tidy_dataframe(\n",
    "    dataframe: DataFrame,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    return dataframe_[\n",
    "        [\n",
    "            \"departure_time\",\n",
    "            \"id\",\n",
    "            \"stop_name\",\n",
    "            \"route_short_name\",\n",
    "            \"stop_id\",\n",
    "            \"direction_id\",\n",
    "        ]\n",
    "   \n",
    "    ]\n",
    "\n",
    "# query the dataset by attribute and value\n",
    "def query(\n",
    "    dataframe: DataFrame,\n",
    "    name: str,\n",
    "    value: Union[str, int, float, Sequence[str]],\n",
    ") -> DataFrame:\n",
    "    return (\n",
    "        dataframe.query(f\"{name} == {value}\")\n",
    "        if isinstance(value, (int, float))\n",
    "        else dataframe.query(f'''{name} == \"{value}\"''')\n",
    "        if isinstance(value, str)\n",
    "        else dataframe.query(f\"{name} in {value}\")\n",
    "    )\n",
    "\n",
    "\n",
    "# filter dataset between two timestamps\n",
    "def between(\n",
    "    dataframe: DataFrame,\n",
    "    start: Union[str, Timestamp],\n",
    "    end: Union[str, Timestamp],\n",
    "    complement: bool = False,\n",
    ") -> DataFrame:\n",
    "    start_ = Timestamp(start) if not isinstance(start, Timestamp) else start\n",
    "    end_ = Timestamp(end) if not isinstance(end, Timestamp) else end\n",
    "    return (\n",
    "        (dataframe.set_index(\"departure_time\").loc[start_:end_].reset_index())\n",
    "        if not complement\n",
    "        else (\n",
    "            dataframe.loc[\n",
    "                (dataframe[\"departure_time\"] < start_)\n",
    "                | (dataframe[\"departure_time\"] > end_)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# intersect two datasets with a common attribute ('on')\n",
    "def intersect(\n",
    "    right: DataFrame,\n",
    "    left: DataFrame,\n",
    "    on: Optional[Sequence[str]] = None,\n",
    "    how: str = \"inner\",\n",
    ") -> Optional[DataFrame]:\n",
    "    on_ = on if on else right.columns.values.tolist()\n",
    "    return pd.merge(\n",
    "        right,\n",
    "        left,\n",
    "        how=how,\n",
    "        on=on_,\n",
    "    )  # if set(rvalues) == set(lvalues) else None\n",
    "\n",
    "\n",
    "# get distinct rows from a dataset grouping by a 'subset'\n",
    "def distinct(\n",
    "    dataframe: DataFrame,\n",
    "    subset: Union[str, Sequence[str]],\n",
    ") -> DataFrame:\n",
    "    return dataframe.drop_duplicates(subset=subset)\n",
    "\n",
    "\n",
    "# count rows by name and value\n",
    "def count_by(\n",
    "    dataframe: DataFrame,\n",
    "    name: str,\n",
    "    value: Union[str, int, float],\n",
    "    *,\n",
    "    frequency: str = \"15T\",\n",
    ") -> DataFrame:\n",
    "    dataframe_ = (\n",
    "        dataframe[dataframe[name] == value]\n",
    "        .set_index(\"departure_time\")\n",
    "        .groupby(\n",
    "            [\n",
    "                pd.Grouper(level=\"departure_time\", freq=frequency),\n",
    "            ]\n",
    "        )\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    # #domain = pd.date_range(start=dataframe_.index.min(), end=dataframe_.index.max(), freq=\"15T\")\n",
    "    # #dataframe_ = dataframe_.reindex(domain, method=None, fill_value=NA)\n",
    "    # #dataframe_.replace(0, np.NAN, inplace=True)\n",
    "    # #display_dataframe(dataframe_)\n",
    "    return dataframe_[dataframe_.columns[0]].to_frame(name=\"count\")\n",
    "\n",
    "\n",
    "# show a timeseries graph of a selected attribute\n",
    "def plot_dataset(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    scatter = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[column],\n",
    "        mode=\"lines\",\n",
    "        name=\"values\",\n",
    "        connectgaps=False,\n",
    "    )\n",
    "\n",
    "    figure.add_trace(scatter)\n",
    "    figure.update_layout(\n",
    "        showlegend=False,\n",
    "        title_text=column,\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(showgrid=True)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of a re-identification attack\n",
    "\n",
    "Somebody said:\n",
    "\n",
    "> \"*I often take the bus in the morning to go to Beaulieu from the 'Anne de Bretagne' in Cesson* \"\n",
    "\n",
    "Is this information enough to discover the mobility patterns of that person?\n",
    "\n",
    "A short summary of implemented methods used to perform the attack, refer to the example below for the use (or if you feel confortable use the **Pandas** API directly):\n",
    "\n",
    "- `query`:  perform a query on the dataset by attribute name and value\n",
    "- `between`:  filter dataset between two timestamps\n",
    "- `intersect`: intersect two datasets with a common attribute (the 'on' attribute)\n",
    "- `distinct`: get distinct rows from a dataset grouping by a 'subset'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove geo-spatial information from the dataset\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# show the dataset\n",
    "print(\"Initial dataset\")\n",
    "display_dataframe(dataset)\n",
    "\n",
    "# query: \"I take the bus from the bus stop 'Anne de Bretagne'\"\n",
    "q_1 = query(dataset, \"stop_name\", \"Anne de Bretagne\")\n",
    "\n",
    "# query: \"I take the bus going to Beaulieu (city center)\"\n",
    "q_2 = query(dataset, \"direction_id\", 0)\n",
    "\n",
    "# intersect results of 'q_1' and 'q_2'\n",
    "q_3 = intersect(q_1, q_2, on=[\"id\"])\n",
    "\n",
    "# show results of intesection done on 'q_3'\n",
    "print(\"Result of the intersection of queries 1 & 2\")\n",
    "display_dataframe(q_3)\n",
    "\n",
    "# check how many different users are in query 'q_3'\n",
    "q_4 = distinct(q_3, [\"id\"])\n",
    "\n",
    "# show results of query 'q_3' \n",
    "# => since there is only one row we found the user!\")\n",
    "print(\"Result of checking different `id` in previous result\")\n",
    "display_dataframe(q_4)\n",
    "\n",
    "# query: all travels of the user ('id') of query 'q_4'\n",
    "q_5 = query(dataset, \"id\", 175)\n",
    "\n",
    "# show results of query 'q_5'\n",
    "print(\"Complete dataset of the user with `id` 175\")\n",
    "display_dataframe(q_5)\n",
    "\n",
    "# get the travels count of the user ('id') of query 'q_3' in a timeline\n",
    "q_6 = count_by(dataset, \"id\", 175)\n",
    "\n",
    "# plot esults of query 'q_6' \n",
    "plot_dataset(q_6, \"count\")\n",
    "\n",
    "# for the curious:\n",
    "# all-in-one 'plain vanilla' code equivalent as follows \n",
    "# (results are not printed on screen)\n",
    "target = dataset.query(\n",
    "    \"stop_name == 'Anne de Bretagne' & direction_id == 0\"\n",
    ").drop_duplicates(\n",
    "    subset=[\n",
    "        \"id\",\n",
    "        \"stop_name\",\n",
    "    ],\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Food for thoughts\n",
    "\n",
    "Here below there is auxiliary information that you have on different targets. Can you re-identify them based on the available dataset? \n",
    "\n",
    "```\n",
    "####################\n",
    "# BEGIN : Answer\n",
    "```\n",
    "\n",
    "> - Target 1: *When I go to work using public transportation, I always take the bus going to the lycée Assomtpion, from the begining of the line*.\n",
    "> - Target 2: *I usually take the bus from 'Saint-Sulpice' but during holidays I stayed at my parents' home and I took the bus '217' a couple of times to go to the campus*.\n",
    "> - Target 3: *I take any bus from the RU Étoile to downtown because I live next to the 'Cimetière de l'Est' and I do not mind to walk*.\n",
    "\n",
    "```\n",
    "# END : Answer\n",
    "####################\n",
    "```\n",
    "\n",
    "Do not forget to visit the Web site of the [STAR](https://www.star.fr/accueil). Specially check the [page](https://m.star.fr/) showing the buses serving at a specific bus stop, and the [page](https://www.star.fr/accueil?tx_pnfstarod_searchdocument[action]=search&tx_pnfstarod_searchdocument[controller]=SearchLines) showing the map/schedule of the bus lines.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####################\n",
    "# BEGIN : Code"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Target 1\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# TODO YOUR code here!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Target 2\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# TODO YOUR code here!\n",
    "\n",
    "# NOTE: To use 'between' set the start and end dates as strings:\n",
    "#       result = between(dataset, \"2021-08-01\", \"2021-08-31\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Target 3\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# TODO YOUR code here!\n",
    "\n",
    "# NOTE: To test several values of an attribute at once, provide a list to query:\n",
    "#       values = [\"Tournebride\", \"Le Mail\", \"Maison d'Accueil\"]\n",
    "#       result = query(dataset, \"stop_name\", values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Code\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Explain the success of the attacks\n",
    "\n",
    "<a id=\"explain\"></a>\n",
    "\n",
    "The success of a re-identification attack depends on the identifying power of the attributes that have been used for the attack. You can display below two measures: the distribution of the [cardinalities of the anonymity sets](#aset) that indicates how much individuals are distinguishable on a given set of attributes, and  the [Shannon entropy](#shannon) that quantifies the amount of information carried by each attribute. See the examples below and then play with anonymity sets by changing the set of attributes on which the anonymity sets are computed. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anonymity Sets\n",
    "<a id=\"aset\"></a>\n",
    "\n",
    "Displaying the cardinalities of the anonymity sets inform about the _re-identifyiability_ of the individuals in the dataset: anonymity sets that have a cardinality equal to 1 contain a single individual, those equal to 2 contain two individuals, etc. Selecting the attributes on which you want to compute the anonymity sets and displaying the resulting cardinalities can thus help you explain the success of your attack. An attacker could also tune the attack by using the most identifying attributes. "
   ],
   "metadata": {
    "id": "b0bECEV09vVw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Food for thought\n",
    "\n",
    "\n",
    "```\n",
    "####################\n",
    "# BEGIN : Answer\n",
    "```\n",
    "\n",
    "> - Which set of attributes is the most identifying ? Can you find it efficiently?\n",
    "> - Would your attacks have have been more successful with other/additional information?\n",
    "\n",
    "```\n",
    "# END : Answer\n",
    "####################\n",
    "```\n",
    "\n",
    "Taking into account the buses validation dataset two kinds of anonymity sets can be computed : \n",
    "\n",
    "1. Anonymity set of validations (rows of the dataset)\n",
    "2. Anonymity set of different users (distinct user identifieres by rows)\n",
    "\n",
    "Let's see some [examples](#aset_examples) and then you can chose [below](#asetplay) the attributes on which you compute the anonymity sets. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute the anonymity set of a 'formated' dataframe\n",
    "def get_anonymity_set(\n",
    "    dataframe: DataFrame,\n",
    "    *,\n",
    "    subset: Optional[Sequence[str]] = None,\n",
    "    distinct: Optional[str] = None,\n",
    "    reindex: bool = False,\n",
    ") -> Series:\n",
    "    \n",
    "    # reset index by including zeroes values\n",
    "    def reset_index(serie: Series) -> Series:\n",
    "        domain = range(1, serie.index.max() + 1)\n",
    "        return serie.reindex(domain, fill_value=0)\n",
    "\n",
    "    # select distinct columns by a defined attribute\n",
    "    def get_distinct(\n",
    "        dataframe: DataFrame,\n",
    "        distinct: Optional[str] = None,\n",
    "        subset: Optional[Sequence[str]] = None,\n",
    "    ) -> DataFrame:\n",
    "        dataframe_ = dataframe.copy()\n",
    "        if distinct:\n",
    "            subset_ = copy.deepcopy(subset)\n",
    "            if subset_:\n",
    "                if distinct not in subset_:\n",
    "                    subset_.append(distinct)\n",
    "            else:\n",
    "                subset_ = [distinct]\n",
    "            dataframe_.drop_duplicates(inplace=True, subset=subset_)\n",
    "\n",
    "        return dataframe_\n",
    "\n",
    "    subset = None if not subset else subset\n",
    "    dataframe_ = get_distinct(dataframe, distinct, subset) if distinct else dataframe.copy()\n",
    "    multiplicity = dataframe_.value_counts(subset=subset)\n",
    "    aset = multiplicity.value_counts().sort_index()\n",
    "    aset = reset_index(aset) if reindex else aset\n",
    "    return (\n",
    "        aset.to_frame()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\n",
    "                \"index\": \"cardinality\",\n",
    "                0: \"occurrences\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# show the anonymity set of a dataframe as a barplot\n",
    "def plot_anonymity_set(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = px.bar(\n",
    "        dataframe,\n",
    "        x=\"cardinality\",\n",
    "        y=\"occurrences\",\n",
    "        color=\"occurrences\",\n",
    "        color_continuous_scale=\"Bluered\",\n",
    "        # template=\"plotly_white\",\n",
    "        title=\"Anonymity Set\",\n",
    "    )\n",
    "\n",
    "    figure.update_coloraxes(showscale=False)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "YKBT8XzLyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"aset_examples\"></a>\n",
    "\n",
    "#### Examples of anonymity sets\n",
    "We now see in detail some anonymity sets of individual attributes and some groups (subsets) of them\n",
    "\n",
    "1. Anomymity set for all attributes [[link]](#aset_e1)\n",
    "2. Anomymity set of the '`id`' attribute [[link]](#aset_e2)\n",
    "3. Anomymity set of the '`stop_name`' attribute [[link]](#aset_e3)\n",
    "4. Anonymity set of the '`route_short_name` and  '`direction_id`' attributes [[link]](#aset_e4)\n",
    "5. Anonymity set of the '`departure_time` attribute [[link]](#aset_e5)<a id=\"aset_e2\"></a>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"aset_e1\"></a>\n",
    "\n",
    "1. Anomymity set for all attributes\n",
    "\n",
    "- **Anonymity set of validations for all attributes of the dataset**\n",
    "\n",
    "  This represents the number different validations (count of rows) on the whole dataset. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get a simplified view of the dataset\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# get anonymity set of validations for all attributes\n",
    "anonymity_set = get_anonymity_set(dataset)\n",
    "\n",
    "print(f\"Anonymity set of validations for all attributes\")\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "uniques = dataset.drop_duplicates()\n",
    "print(f\"Occurences of the FIRST cardinality: {uniques.shape[0]}\")\n",
    "display_dataframe(uniques)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Anonymity set of different users for all attributes of the dataset**\n",
    "\n",
    "  This represents the number of diferent users in the dataset (unique identifiers)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get anonymity set of different uses for all attributes\n",
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\")\n",
    "\n",
    "print(f\"Anonymity set of different users for all attributes\")\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "uniques = dataset.drop_duplicates(\"id\")\n",
    "print(f\"Occurrences of the FIRST cardinality: {uniques.shape[0]}\")\n",
    "display_dataframe(uniques)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"aset_e2\"></a>\n",
    "\n",
    "2. Anonymity set of the '`id`' attribute\n",
    "\n",
    "- **Anonymity set of validations for the subset `['id']`**\n",
    "\n",
    "  This represents the number validations (count of rows) for the same unique identifier. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "SUBSET = [\"id\"]\n",
    "\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "print(f\"Anonymity set of validations for {SUBSET=}\")\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.groupby(SUBSET)\n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)\n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "result = dataset[dataset[\"id\"] == rows[\"id\"][0]] \n",
    "print(f\"Occurrences of the FIRST cardinality: {result.shape[0]}\")\n",
    "display_dataframe(result)\n",
    "\n",
    "uniques = result.drop_duplicates(subset=SUBSET)\n",
    "print(f\"Cardinality of the previous occurence (unique rows with the subset): {uniques.shape[0]}\")\n",
    "display_dataframe(uniques)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Anonymity set of different users for the subset `['id']`**\n",
    "\n",
    "  This represents the number of diferent users in the dataset as well!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "print(f\"Anonymity set of different users for {SUBSET=}\")\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "uniques = dataset.drop_duplicates(subset=SUBSET)\n",
    "print(f\"Occurences of the FIRST cardinality: {uniques.shape[0]}\")\n",
    "display_dataframe(uniques)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"aset_e3\"></a>\n",
    "\n",
    "3. Anonymity set of the '`stop_name`' attribute\n",
    "\n",
    "- **Anonymity set of validations for the subset `['stop_name']`**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "SUBSET = [\"stop_name\"]\n",
    "\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "print(f\"Anonymity set of validations for {SUBSET=}\")\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.groupby(SUBSET)\n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)\n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "result = dataset[dataset[\"stop_name\"] == rows[\"stop_name\"][0]] \n",
    "print(f\"Occurrences of the FIRST cardinality: {result.shape[0]}\")\n",
    "display_dataframe(result)\n",
    "\n",
    "uniques = result.drop_duplicates(subset=SUBSET)\n",
    "print(f\"Cardinality of the previous occurence (unique rows with the subset): {uniques.shape[0]}\")\n",
    "display_dataframe(uniques)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **Anonymity set of different users for the subset `['stop_name']`**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "print(f\"Anonymity set of different users for {SUBSET=}\")\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.drop_duplicates(subset=SUBSET + [\"id\"])\n",
    "    .groupby(SUBSET + [\"id\"])    \n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)    \n",
    "    .groupby(SUBSET)\n",
    "    .count()  \n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index() \n",
    ")\n",
    "\n",
    "# def flat(lista):\n",
    "#     return set(item for sublist in lista for item in sublist)\n",
    "\n",
    "# groups = (\n",
    "#     dataset.drop_duplicates(subset=SUBSET + [\"id\"])\n",
    "#     .groupby(SUBSET + [\"id\"])\n",
    "#     .aggregate(lambda x: list(x))\n",
    "#     .groupby(SUBSET)\n",
    "#     .aggregate(lambda x: flat(x))\n",
    "# )\n",
    "\n",
    "#display_dataframe(groups)\n",
    "    \n",
    "cardinality = rows[rows[\"count\"] == rows[\"count\"][0]]\n",
    "print(f\"Occurrences of the FIRST cardinality: {cardinality.shape[0]}\")\n",
    "display_dataframe(cardinality)\n",
    "\n",
    "# get first element's data of the cardinality\n",
    "result = dataset[dataset[\"stop_name\"] == cardinality[\"stop_name\"][0]] \n",
    "print(f\"Dataset of the FIRST occurrence\")\n",
    "display_dataframe(result)\n",
    "\n",
    "uniques = result.drop_duplicates(subset=SUBSET+ [\"id\"])\n",
    "print(f\"Cardinality of the previous dataset (unique rows with the subset): {uniques.shape[0]}\")\n",
    "display_dataframe(uniques)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"aset_e4\"></a>\n",
    "\n",
    "\n",
    "4. Anonymity set of the '`route_short_name` and  '`direction_id`' attributes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "SUBSET = [\n",
    "    \"route_short_name\",\n",
    "    \"direction_id\",\n",
    "]\n",
    "\n",
    "### ANONIMITY SET OF VALIDATIONS\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.groupby(SUBSET)\n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)\n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "result = dataset[\n",
    "    (dataset[\"route_short_name\"] == rows[\"route_short_name\"][0])\n",
    "    & (dataset[\"direction_id\"] == rows[\"direction_id\"][0])\n",
    "]\n",
    "\n",
    "display_dataframe(result)\n",
    "\n",
    "### ANONIMITY SET OF USERS\n",
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.drop_duplicates(subset=SUBSET + [\"id\"])\n",
    "    .groupby(SUBSET + [\"id\"])    \n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)    \n",
    "    .groupby(SUBSET)\n",
    "    .count()  \n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index() \n",
    ")\n",
    "\n",
    "# get first cardinality \n",
    "cardinality = rows[rows[\"count\"] == rows[\"count\"][0]]\n",
    "display_dataframe(cardinality)\n",
    "\n",
    "# get first element's data of the cardinality\n",
    "result = dataset[\n",
    "    (dataset[\"route_short_name\"] == cardinality[\"route_short_name\"][0])\n",
    "    & (dataset[\"direction_id\"] == cardinality[\"direction_id\"][0])\n",
    "]\n",
    "\n",
    "# check that the result query correspond to the cardinality\n",
    "display_dataframe(result.drop_duplicates(subset=SUBSET + [\"id\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"aset_e5\"></a>\n",
    "\n",
    "5. Anonymity set of the '`departure_time`' attribute"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "SUBSET = [            \n",
    "    \"departure_time\",    \n",
    "]\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "# Question: Why they are equal ? ;)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"asetplay\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# (un)comment lines starting with dash ('#') to change the subset\n",
    "\n",
    "####################\n",
    "# BEGIN : Play\n",
    "SUBSET = [\n",
    "    #\"departure_time\",\n",
    "    #\"id\",\n",
    "    #\"stop_name\",\n",
    "    #\"route_short_name\",\n",
    "    #\"stop_id\",\t\n",
    "    #\"direction_id\",\n",
    "]\n",
    "# END : Play\n",
    "####################\n",
    "\n",
    "# get a simplified view of the dataset\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "### ANONIMITY SET OF VALIDATIONS\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "### ANONIMITY SET OF USERS\n",
    "anonymity_set = get_anonymity_set(dataset, distinct= \"id\", subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shannon's entropy\n",
    "<a id=\"shannon\"></a>\n",
    "\n",
    "TODO texte Shannon \n",
    "\n",
    "Food for thought : \n",
    "- Which attributes give the most information ?\n",
    "- Would your attacks have have been more successful with other/additional information ?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute the entropy of a serie\n",
    "def entropy(\n",
    "    series: Series,\n",
    "    base: int = 2,\n",
    "    normalize: bool = False,\n",
    ") -> float:\n",
    "    # compute the expectation of a serie\n",
    "    def expectation(probability: Series) -> float:\n",
    "        return (probability * np.log(probability) / np.log(base)).sum()\n",
    "\n",
    "    # compute the efficiency of a serie\n",
    "    def efficiency(entropy: float, length: int) -> float:\n",
    "        return entropy * np.log(base) / np.log(length)\n",
    "\n",
    "    probability = series.value_counts(normalize=True, sort=False)\n",
    "    h = -expectation(probability)\n",
    "    return efficiency(h, series.size) if normalize else h\n",
    "\n",
    "\n",
    "# compute the entropy of a dataframe\n",
    "def get_entropies(\n",
    "    dataframe: DataFrame,\n",
    "    base: int = 2,\n",
    "    normalize: bool = False,\n",
    ") -> Series:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    entropies = dataframe_.apply(\n",
    "        entropy,\n",
    "        base=base,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        entropies.to_frame()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\n",
    "                \"index\": \"attribute\",\n",
    "                0: \"entropy\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# show the entropies as a dataframe as barplot\n",
    "def plot_entropies(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = px.bar(\n",
    "        dataframe,\n",
    "        x=\"entropy\",\n",
    "        y=\"attribute\",\n",
    "        orientation=\"h\",\n",
    "        color=\"attribute\",\n",
    "    )\n",
    "\n",
    "    figure.update_traces(\n",
    "        texttemplate=\"%{x:.2f}\",\n",
    "        textposition=\"auto\",\n",
    "    )\n",
    "\n",
    "    figure.update_layout(showlegend=False)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get a simplified view of the dataset\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# show the dataset\n",
    "display_dataframe(dataset)\n",
    "\n",
    "# compute the entropies of the dataset\n",
    "entropies = get_entropies(dataset, normalize=True)\n",
    "\n",
    "# show a barplot of the entropies \n",
    "plot_entropies(entropies)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NmKSCNfpfOQ-",
    "qzLkzFP3fZm-",
    "O4teYxHrfnuh",
    "-kXTLBmBfxuL",
    "ZV4MJqgogTB0",
    "F-CFufprhULZ",
    "b5Hiwx5O7gXw",
    "tawuGE-n9PTw",
    "s4gBkGhqpaL7"
   ],
   "include_colab_link": true,
   "name": "a2r2-notebook02.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "d5511370cecfc9f72087461b207d0bd90f18099e89758b2a61eb3e3243f66294"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit ('venv-a2r2': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}