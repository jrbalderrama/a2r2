{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrbalderrama/a2r2/blob/main/notebooks/a2r2-02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUDI Workshop: Introduction to Privacy-Preserving Data Publishing Techniques\n",
    "\n",
    "Tristan ALLARD & Javier ROJAS BALDERRAMA\n",
    "\n",
    "_Univ Rennes, CNRS, INRIA_\n",
    "  \n",
    "This work is licensed under a [Creative Commons Zero v1.0 Universal License](https://creativecommons.org/publicdomain/zero/1.0/)"
   ],
   "metadata": {
    "id": "5qOVUmdqyTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook __TWO__: The case for privacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preamble\n",
    "\n",
    "Yes, raw data is not immune to re-identification! \n",
    "\n",
    "You are now going to perform a reidentification attack on a small set of targets. To this end, we will give you some auxiliary information (also called background knowledge) and programming tools for helping you query the dataset.\n",
    "1. You can display the buses validations dataset [here](#displayvalid). Feel free to to play with the filter menu,although the number of shown rows is limited. \n",
    "2. You can attack the dataset [Step 1](#attack) (do not be afraid to try!). \n",
    "3. In order to understand better your attacks and/or design other attacks, you can display informative measures about the _identifying power_ of the attributes of the dataset ([Step 2](#explain)). "
   ],
   "metadata": {
    "id": "OD991wF88Rio"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    " ### Download dataset\n"
   ],
   "metadata": {
    "id": "qzLkzFP3fZm-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -nv -nc https://zenodo.org/record/5509268/files/buses.parquet"
   ],
   "outputs": [],
   "metadata": {
    "id": "u_Eju0G4yTjY",
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import required modules"
   ],
   "metadata": {
    "id": "O4teYxHrfnuh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import copy\n",
    "import importlib\n",
    "import os\n",
    "from errno import ENOENT\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pyarrow.parquet as pq\n",
    "from folium.plugins import HeatMapWithTime\n",
    "from IPython import display, get_ipython\n",
    "from numpy import ndarray\n",
    "from pandas import NA, DataFrame, DatetimeIndex, Series, Timedelta, Timestamp\n",
    "from plotly.graph_objs import Bar, Figure, Layout, Scatter"
   ],
   "outputs": [],
   "metadata": {
    "id": "PLHjpQH6yTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup notebook constants and running environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# project base directory\n",
    "BASE_DIRECTORY = Path(\".\")\n",
    "\n",
    "# detect running environment\n",
    "COLAB_ON = True if \"google.colab\" in str(get_ipython()) else False"
   ],
   "outputs": [],
   "metadata": {
    "id": "fToRyDS0yTjY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set Ploty renderer\n",
    "if COLAB_ON:\n",
    "    pio.renderers.default = \"colab\""
   ],
   "outputs": [],
   "metadata": {
    "id": "82e_w9dyyTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and display raw dataset"
   ],
   "metadata": {
    "id": "s4gBkGhqpaL7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load dataset from file system\n",
    "def load_data(\n",
    "    path: Path,\n",
    ") -> DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(ENOENT, os.strerror(ENOENT), path)\n",
    "\n",
    "    table = pq.read_table(path)\n",
    "    return table.to_pandas()\n",
    "\n",
    "\n",
    "# show a dataframe as a table\n",
    "def display_dataframe(\n",
    "        dataframe: DataFrame,\n",
    ") -> None:    \n",
    "    if COLAB_ON:\n",
    "        spec = importlib.util.find_spec(\"google.colab\")\n",
    "        if spec:            \n",
    "            data_table = importlib.import_module(\"google.colab.data_table\")            \n",
    "            enable_dataframe_formatter = getattr(\n",
    "                data_table, \n",
    "                \"enable_dataframe_formatter\",\n",
    "            )            \n",
    "            \n",
    "            enable_dataframe_formatter()            \n",
    "           \n",
    "    display.display(dataframe[:20000] if COLAB_ON else dataframe) "
   ],
   "outputs": [],
   "metadata": {
    "id": "iUo9bi14yTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Show raw dataset\n",
    "\n",
    "<a id=\"displayvalid\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = BASE_DIRECTORY.joinpath(\"buses.parquet\")\n",
    "buses_dataset = load_data(path)\n",
    "display_dataframe(buses_dataset)\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Observe\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show dataset on a map\n",
    "def plot_heatmap(\n",
    "    dataframe: DataFrame,\n",
    "    group_column: str = \"departure_time\",\n",
    "    # Rennes GPS coordinates\n",
    "    location: Tuple[float, float] = (48.1147, -1.6794),\n",
    ") -> None:\n",
    "    _dataframe = dataframe.copy(deep=True)\n",
    "    timestamps = []\n",
    "    coordinates = []\n",
    "    for timestamp, coordinate in _dataframe.groupby(group_column):\n",
    "        timestamps.append(str(timestamp))\n",
    "        coordinates.append(\n",
    "            coordinate[\n",
    "                [\n",
    "                    \"stop_lat\",\n",
    "                    \"stop_lon\",\n",
    "                ]\n",
    "            ].values.tolist()\n",
    "        )\n",
    "\n",
    "    base_map = folium.Map(\n",
    "        location=location,\n",
    "        zoom_start=11,\n",
    "        tiles=\"https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png\",\n",
    "        # tiles=\"https://{s}.basemaps.cartocdn.com/dark_nolabels/{z}/{x}/{y}{r}.png\",\n",
    "        attr=\"CartoDB\",\n",
    "    )\n",
    "\n",
    "    heat_map = HeatMapWithTime(\n",
    "        data=coordinates,\n",
    "        index=timestamps,\n",
    "        auto_play=True,\n",
    "        min_speed=1,\n",
    "        radius=4,\n",
    "        max_opacity=0.5,\n",
    "    )\n",
    "\n",
    "    heat_map.add_to(base_map)\n",
    "    display.display(base_map)"
   ],
   "outputs": [],
   "metadata": {
    "id": "p-1jePEeyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**:\n",
    "\n",
    "> Showing the heat map of the buses validation only works on a local\n",
    "> Jupyter server (a *colab* feature/limitation)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#plot_heatmap(buses_dataset)\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ],
   "outputs": [],
   "metadata": {
    "id": "KwNpJzamyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Observe\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Attack raw buses validations\n",
    "<a id=\"attack\"></a>\n",
    "\n",
    "Re-identification attacks are simple conceptually. They consist in selecting the subset of individuals whose records match the auxiliary information that the attacker has about them. If a single individual matches the adversarial knowledge, the success of the attack is clear (assuming that the adversarial knowledge is reliable). But when more than a single individual match the adversarial knowledge, is it a failure? \n",
    "\n",
    "By \"looking\" at the dataset (see above), could you imagine stronger auxiliary information?"
   ],
   "metadata": {
    "id": "65WnAyybppNH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# drop geospatial attributes from dataset\n",
    "def tidy_dataframe(\n",
    "    dataframe: DataFrame,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    return dataframe_[\n",
    "        [\n",
    "            \"departure_time\",\n",
    "            \"id\",\n",
    "            \"stop_name\",\n",
    "            \"route_short_name\",\n",
    "            \"stop_id\",\n",
    "            \"direction_id\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "def query(\n",
    "    dataframe: DataFrame,\n",
    "    name: str,\n",
    "    value: Union[str, int, float, Sequence[str]],\n",
    ") -> DataFrame:\n",
    "    return (\n",
    "        dataframe.query(f\"{name} == {value}\")\n",
    "        if isinstance(value, (int, float))\n",
    "        else dataframe.query(f\"{name} == '{value}'\")\n",
    "        if isinstance(value, str)\n",
    "        else dataframe.query(f\"{name} in {value}\")\n",
    "    )\n",
    "\n",
    "\n",
    "def between(\n",
    "    dataframe: DataFrame,\n",
    "    start: Union[str, Timestamp],\n",
    "    end: Union[str, Timestamp],\n",
    "    complement: bool = False,\n",
    ") -> DataFrame:\n",
    "    start_ = Timestamp(start) if not isinstance(start, Timestamp) else start\n",
    "    end_ = Timestamp(end) if not isinstance(end, Timestamp) else end\n",
    "    return (\n",
    "        (dataframe.set_index(\"departure_time\").loc[start_:end_].reset_index())\n",
    "        if not complement\n",
    "        else (\n",
    "            dataframe.loc[\n",
    "                (dataframe[\"departure_time\"] < start_)\n",
    "                | (dataframe[\"departure_time\"] > end_)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def intersect(\n",
    "    right: DataFrame,\n",
    "    left: DataFrame,\n",
    "    on: Optional[Sequence[str]] = None,\n",
    "    how: str = \"inner\",\n",
    ") -> Optional[DataFrame]:\n",
    "    on_ = on if on else right.columns.values.tolist()\n",
    "    return pd.merge(\n",
    "        right,\n",
    "        left,\n",
    "        how=how,\n",
    "        on=on_,\n",
    "    )  # if set(rvalues) == set(lvalues) else None\n",
    "\n",
    "\n",
    "def distinct(\n",
    "    dataframe: DataFrame,\n",
    "    subset: Union[str, Sequence[str]],\n",
    ") -> DataFrame:\n",
    "    return dataframe.drop_duplicates(subset=subset)\n",
    "\n",
    "\n",
    "def count_by(\n",
    "    dataframe: DataFrame,\n",
    "    name: str,\n",
    "    value: Union[str, int, float],\n",
    "    *,\n",
    "    frequency: str = \"15T\",\n",
    ") -> DataFrame:\n",
    "    dataframe_ = (\n",
    "        dataframe[dataframe[name] == value]\n",
    "        .set_index(\"departure_time\")\n",
    "        .groupby(\n",
    "            [\n",
    "                pd.Grouper(level=\"departure_time\", freq=frequency),\n",
    "            ]\n",
    "        )\n",
    "        .count()\n",
    "    )\n",
    "\n",
    "    # #domain = pd.date_range(start=dataframe_.index.min(), end=dataframe_.index.max(), freq=\"15T\")\n",
    "    # #dataframe_ = dataframe_.reindex(domain, method=None, fill_value=NA)\n",
    "    # #dataframe_.replace(0, np.NAN, inplace=True)\n",
    "    # #display_dataframe(dataframe_)\n",
    "    return dataframe_[dataframe_.columns[0]].to_frame(name=\"count\")\n",
    "\n",
    "\n",
    "# show a timeseries graph of a selected attribute\n",
    "def plot_dataset(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    scatter = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[column],\n",
    "        mode=\"lines\",\n",
    "        name=\"values\",\n",
    "        connectgaps=False,\n",
    "    )\n",
    "\n",
    "    figure.add_trace(scatter)\n",
    "    figure.update_layout(\n",
    "        showlegend=False,\n",
    "        title_text=column,\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(showgrid=True)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of a re-identification attack\n",
    "\n",
    "Somebody said:\n",
    "\n",
    "> I frequently take the bus in the morning to go to Beaulieu from the 'Anne de Bretagne' in Cesson\n",
    "\n",
    "Can this information is enough to discover the mobility patterns of that person?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove geo-spatial information from the dataset\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "display_dataframe(dataset)\n",
    "\n",
    "# I take the bus from the bus stop 'Anne de Bretagne'\n",
    "q_1 = query(dataset, \"stop_name\", \"Anne de Bretagne\")\n",
    "\n",
    "# The bus of 'Anne de Bretagne' (Cesson) goes to Rennes city center (Beaulieu)\n",
    "# Note: in 'direction_id' aller to Rennes is 0 & retour from Rennes is 1\n",
    "q_2 = query(q_1, \"direction_id\", 0)\n",
    "\n",
    "print(\"Results of query #2\")\n",
    "display_dataframe(q_2)\n",
    "\n",
    "# check how many different users are in query #2\n",
    "q_3 = distinct(q_2, [\"id\"])\n",
    "\n",
    "print(\"Results of query #3 -> Found!\")\n",
    "display_dataframe(q_3)\n",
    "\n",
    "# Get all travels of the user of query #3\n",
    "q_5 = query(dataset, \"id\", 175)\n",
    "\n",
    "print(\"Results of query #5\")\n",
    "display_dataframe(q_5)\n",
    "\n",
    "# show travels of the user of query #3 by day\n",
    "count = count_by(dataset, \"id\", 175)\n",
    "plot_dataset(count, \"count\")\n",
    "\n",
    "\n",
    "# For the curious: All-in-one 'plain vanilla' code equivalent:\n",
    "result = dataset.query(\n",
    "    \"stop_name == 'Anne de Bretagne' & direction_id == 0\"\n",
    ").drop_duplicates(\n",
    "    subset=[\n",
    "        \"id\",\n",
    "        \"stop_name\",\n",
    "    ],\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Food for thoughts\n",
    "\n",
    "Here is below auxiliary information that you have on different targets. Can you re-identify them based on the available dataset? \n",
    "\n",
    "```\n",
    "####################\n",
    "# BEGIN : Answer\n",
    "```\n",
    "\n",
    "> - Target 1: When I go to work using public transportation, I always take the bus going to the lycée Assomtpion, from the begining of the line.\n",
    "> - Target 2: I usually take the bus from 'Saint-Sulpice' but during holidays I stayed at my parents' home and I took the bus '217' a couple of times to go to the campus\n",
    "> - Target 3: I take any bus from the RU Étoile to downtown because I live next to the 'Cimetière de l'Est'\n",
    "\n",
    "```\n",
    "# END : Answer\n",
    "####################\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Target 1\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# TODO YOUR code comes here!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Target 2\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# TODO YOUR code comes here!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Target 3\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# TODO YOUR code comes here!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Code\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Explain the success of the attacks\n",
    "\n",
    "<a id=\"explain\"></a>\n",
    "\n",
    "The success of a re-identification attack depends on the identifying power of the attributes that have been used for the attack. You can display below two measures: the [Shannon entropy](#shannon) (that quantifies the amount of information carried by each attribute) and the distribution of the [cardinalities of the anonymity sets](#aset) (that indicates how much individuals are distinguishable on a given set of attributes). Do not hesitate to play with anonymity sets by changing the set of attributes on which the anonymity sets are computed. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shannon's entropy\n",
    "<a id=\"shannon\"></a>\n",
    "\n",
    "TODO texte Shannon \n",
    "\n",
    "Food for thought : \n",
    "- Which attributes give the most information ?\n",
    "- Would your attacks have have been more successful with other/additional information ?"
   ],
   "metadata": {
    "id": "1OHcTzRI9oQo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute the entropy of a serie\n",
    "def entropy(\n",
    "    series: Series,\n",
    "    base: int = 2,\n",
    "    normalize: bool = False,\n",
    ") -> float:\n",
    "    # compute the expectation of a serie\n",
    "    def expectation(probability: Series) -> float:\n",
    "        return (probability * np.log(probability) / np.log(base)).sum()\n",
    "\n",
    "    # compute the efficiency of a serie\n",
    "    def efficiency(entropy: float, length: int) -> float:\n",
    "        return entropy * np.log(base) / np.log(length)\n",
    "\n",
    "    probability = series.value_counts(normalize=True, sort=False)\n",
    "    h = -expectation(probability)\n",
    "    return efficiency(h, series.size) if normalize else h\n",
    "\n",
    "\n",
    "# compute the entropy of a dataframe\n",
    "def get_entropies(\n",
    "    dataframe: DataFrame,\n",
    "    base: int = 2,\n",
    "    normalize: bool = False,\n",
    ") -> Series:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    entropies = dataframe_.apply(\n",
    "        entropy,\n",
    "        base=base,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        entropies.to_frame()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\n",
    "                \"index\": \"attribute\",\n",
    "                0: \"entropy\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# show the entropies as a dataframe as barplot\n",
    "def plot_entropies(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = px.bar(\n",
    "        dataframe,\n",
    "        x=\"entropy\",\n",
    "        y=\"attribute\",\n",
    "        orientation=\"h\",\n",
    "        color=\"attribute\",\n",
    "    )\n",
    "\n",
    "    figure.update_traces(\n",
    "        texttemplate=\"%{x:.2f}\",\n",
    "        textposition=\"auto\",\n",
    "    )\n",
    "\n",
    "    figure.update_layout(showlegend=False)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "AS66DfaDyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get a simplified view of the dataset\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "# show the dataset\n",
    "display_dataframe(dataset)\n",
    "\n",
    "# compute the entropies of the dataset\n",
    "entropies = get_entropies(dataset, normalize=True)\n",
    "\n",
    "# show a barplot of the entropies \n",
    "plot_entropies(entropies)"
   ],
   "outputs": [],
   "metadata": {
    "id": "ckEdKVZZyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anonymity Sets\n",
    "<a id=\"aset\"></a>\n",
    "\n",
    "Displaying the cardinalities of the anonymity sets inform about the _re-identifyiability_ of the individuals in the dataset: anonymity sets that have a cardinality equal to 1 contain a single individual, those equal to 2 contain two individuals, _etc_. Selecting the attributes on which you want to compute the anonymity sets and displaying the resulting cardinalities can thus help you explain the success of your attack. An attacker could also tune the attack by using the most identifying attributes. \n",
    "\n",
    "You can chose [below](#asetplay) the attributes on which you compute the anonymity sets. \n",
    "\n",
    "Food for thought : \n",
    "- Which set of attributes is the most identifying ? Can you find it efficiently ?\n",
    "- Would your attacks have have been more successful with other/additional information ?"
   ],
   "metadata": {
    "id": "b0bECEV09vVw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute the anonymity set of a 'formated' dataframe\n",
    "def get_anonymity_set(\n",
    "    dataframe: DataFrame,\n",
    "    *,\n",
    "    subset: Optional[Sequence[str]] = None,\n",
    "    distinct: Optional[str] = None,\n",
    "    reindex: bool = False,\n",
    ") -> Series:\n",
    "    \n",
    "    # reset index by including zeroes values\n",
    "    def reset_index(serie: Series) -> Series:\n",
    "        domain = range(1, serie.index.max() + 1)\n",
    "        return serie.reindex(domain, fill_value=0)\n",
    "\n",
    "    # select distinct columns by a defined attribute\n",
    "    def get_distinct(\n",
    "        dataframe: DataFrame,\n",
    "        distinct: Optional[str] = None,\n",
    "        subset: Optional[Sequence[str]] = None,\n",
    "    ) -> DataFrame:\n",
    "        dataframe_ = dataframe.copy()\n",
    "        if distinct:\n",
    "            subset_ = copy.deepcopy(subset)\n",
    "            if subset_:\n",
    "                if distinct not in subset_:\n",
    "                    subset_.append(distinct)\n",
    "            else:\n",
    "                subset_ = [distinct]\n",
    "            dataframe_.drop_duplicates(inplace=True, subset=subset_)\n",
    "\n",
    "        return dataframe_\n",
    "\n",
    "    subset = None if not subset else subset\n",
    "    dataframe_ = get_distinct(dataframe, distinct, subset) if distinct else dataframe.copy()\n",
    "    multiplicity = dataframe_.value_counts(subset=subset)\n",
    "    aset = multiplicity.value_counts().sort_index()\n",
    "    aset = reset_index(aset) if reindex else aset\n",
    "    return (\n",
    "        aset.to_frame()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\n",
    "                \"index\": \"cardinality\",\n",
    "                0: \"occurrences\",\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# show the anonymity set of a dataframe as a barplot\n",
    "def plot_anonymity_set(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = px.bar(\n",
    "        dataframe,\n",
    "        x=\"cardinality\",\n",
    "        y=\"occurrences\",\n",
    "        color=\"occurrences\",\n",
    "        color_continuous_scale=\"Bluered\",\n",
    "        # template=\"plotly_white\",\n",
    "        title=\"Anonymity Set\",\n",
    "    )\n",
    "\n",
    "    figure.update_coloraxes(showscale=False)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "YKBT8XzLyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"asetplay\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####################\n",
    "# BEGIN : Play\n",
    "\n",
    "# define a subset of specific attributes to take into account\n",
    "# uncomment lines starting with dash ('#') to change the subset\n",
    "SUBSET = [\n",
    "    #\"departure_time\",\n",
    "    #\"id\",\n",
    "    #\"stop_name\",\n",
    "    #\"route_short_name\",\n",
    "    #\"stop_id\",\t\n",
    "    #\"direction_id\",\n",
    "]\n",
    "# END : Play\n",
    "####################\n",
    "\n",
    "# get a simplified view of the dataset\n",
    "dataset = tidy_dataframe(buses_dataset)\n",
    "\n",
    "### ANONIMITY SET OF VALIDATIONS\n",
    "# anonymity set of validations (different rows) for a given subset\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "\n",
    "# show a barplot of the entropies by validations\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "# the anonymity set represent unique rows in the whole dataset\n",
    "# it is equivalent to the following query:\n",
    "uniques = dataset.drop_duplicates()\n",
    "display_dataframe(uniques)\n",
    "\n",
    "### ANONIMITY SET OF USERS\n",
    "# anonymity set of usesr (distinct IDs) for a given subset\n",
    "anonymity_set = get_anonymity_set(dataset, distinct= \"id\", subset=SUBSET)\n",
    "\n",
    "# show a barplot of the entropies by user\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "# the anonymity set represent unique rows of different users\n",
    "# it is equivalent to the following query:\n",
    "uniques = dataset.drop_duplicates(subset=[\"id\"])\n",
    "display_dataframe(uniques)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Additional examples with some explanations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "SUBSET = [\"id\"]\n",
    "\n",
    "### ANONIMITY SET OF VALIDATIONS\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "# the anonymity set represent the count of rows for the same ID\n",
    "rows = (\n",
    "    dataset.groupby(SUBSET)\n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)\n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "result = dataset[dataset[\"id\"] == rows[\"id\"][0]] \n",
    "display_dataframe(result)\n",
    "\n",
    "# check that the result query correspond to the cardinality\n",
    "display_dataframe(result.drop_duplicates(subset=SUBSET))\n",
    "\n",
    "### ANONIMITY SET OF USERS\n",
    "# this case is equivalent to:\n",
    "# dataset.drop_duplicates(subset=[\"id\"])\n",
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "SUBSET = [\"stop_name\"]\n",
    "\n",
    "### ANONIMITY SET OF VALIDATIONS\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.groupby(SUBSET)\n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)\n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "result = dataset[dataset[\"stop_name\"] == rows[\"stop_name\"][0]] \n",
    "display_dataframe(result)\n",
    "\n",
    "### ANONIMITY SET OF USERS\n",
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.drop_duplicates(subset=SUBSET + [\"id\"])\n",
    "    .groupby(SUBSET + [\"id\"])    \n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)    \n",
    "    .groupby(SUBSET)\n",
    "    .count()  \n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index() \n",
    ")\n",
    "\n",
    "# def flat(lista):\n",
    "#     return set(item for sublist in lista for item in sublist)\n",
    "\n",
    "# groups = (\n",
    "#     dataset.drop_duplicates(subset=SUBSET + [\"id\"])\n",
    "#     .groupby(SUBSET + [\"id\"])\n",
    "#     .aggregate(lambda x: list(x))\n",
    "#     .groupby(SUBSET)\n",
    "#     .aggregate(lambda x: flat(x))\n",
    "# )\n",
    "\n",
    "#display_dataframe(groups)\n",
    "    \n",
    "# get first cardinality \n",
    "cardinality = rows[rows[\"count\"] == rows[\"count\"][0]]\n",
    "display_dataframe(cardinality)\n",
    "\n",
    "# get first element's data of the cardinality\n",
    "result = dataset[dataset[\"stop_name\"] == cardinality[\"stop_name\"][0]] \n",
    "display_dataframe(result)\n",
    "\n",
    "# check that the result query correspond to the cardinality\n",
    "display_dataframe(result.drop_duplicates(subset=SUBSET + [\"id\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "SUBSET = [\n",
    "    \"route_short_name\",\n",
    "    \"direction_id\",\n",
    "]\n",
    "\n",
    "### ANONIMITY SET OF VALIDATIONS\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.groupby(SUBSET)\n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)\n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "result = dataset[\n",
    "    (dataset[\"route_short_name\"] == rows[\"route_short_name\"][0])\n",
    "    & (dataset[\"direction_id\"] == rows[\"direction_id\"][0])\n",
    "]\n",
    "\n",
    "display_dataframe(result)\n",
    "\n",
    "### ANONIMITY SET OF USERS\n",
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "rows = (\n",
    "    dataset.drop_duplicates(subset=SUBSET + [\"id\"])\n",
    "    .groupby(SUBSET + [\"id\"])    \n",
    "    .agg({\"stop_id\": \"count\"})\n",
    "    .rename({\"stop_id\": \"count\"}, axis=1)    \n",
    "    .groupby(SUBSET)\n",
    "    .count()  \n",
    "    .sort_values(by=\"count\")\n",
    "    .reset_index() \n",
    ")\n",
    "\n",
    "# get first cardinality \n",
    "cardinality = rows[rows[\"count\"] == rows[\"count\"][0]]\n",
    "display_dataframe(cardinality)\n",
    "\n",
    "# get first element's data of the cardinality\n",
    "result = dataset[\n",
    "    (dataset[\"route_short_name\"] == cardinality[\"route_short_name\"][0])\n",
    "    & (dataset[\"direction_id\"] == cardinality[\"direction_id\"][0])\n",
    "]\n",
    "\n",
    "# check that the result query correspond to the cardinality\n",
    "display_dataframe(result.drop_duplicates(subset=SUBSET + [\"id\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = tidy_dataframe(buses_dataset)\n",
    "SUBSET = [            \n",
    "    \"departure_time\",    \n",
    "]\n",
    "anonymity_set = get_anonymity_set(dataset, subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "anonymity_set = get_anonymity_set(dataset, distinct=\"id\", subset=SUBSET)\n",
    "plot_anonymity_set(anonymity_set)\n",
    "\n",
    "# Question: Why they are equal ? ;)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NmKSCNfpfOQ-",
    "qzLkzFP3fZm-",
    "O4teYxHrfnuh",
    "-kXTLBmBfxuL",
    "ZV4MJqgogTB0",
    "F-CFufprhULZ",
    "b5Hiwx5O7gXw",
    "tawuGE-n9PTw",
    "s4gBkGhqpaL7"
   ],
   "include_colab_link": true,
   "name": "a2r2-notebook02.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "d5511370cecfc9f72087461b207d0bd90f18099e89758b2a61eb3e3243f66294"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit ('venv-a2r2': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}