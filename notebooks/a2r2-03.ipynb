{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrbalderrama/a2r2/blob/main/notebooks/a2r2-03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUDI Workshop: Introduction to Privacy-Preserving Data Publishing Techniques\n",
    "\n",
    "Tristan ALLARD & Javier ROJAS BALDERRAMA\n",
    "\n",
    "_Univ Rennes, CNRS, INRIA_\n",
    "  \n",
    "This work is licensed under a [Creative Commons Zero v1.0 Universal License](https://creativecommons.org/publicdomain/zero/1.0/)"
   ],
   "metadata": {
    "id": "5qOVUmdqyTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook __THREE__: Protection with differential privacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 0 (PREAMBLE): Settings and data"
   ],
   "metadata": {
    "id": "NmKSCNfpfOQ-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    " ### Download datasets\n"
   ],
   "metadata": {
    "id": "qzLkzFP3fZm-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -nv -nc https://zenodo.org/record/5509313/files/classes.parquet\n",
    "!wget -nv -nc https://zenodo.org/record/5509268/files/buses.parquet"
   ],
   "outputs": [],
   "metadata": {
    "id": "u_Eju0G4yTjY",
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Import required modules"
   ],
   "metadata": {
    "id": "O4teYxHrfnuh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "import importlib\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "from errno import ENOENT\n",
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from IPython import display, get_ipython\n",
    "from numpy import linalg, ndarray\n",
    "from pandas import NA, DataFrame, DatetimeIndex, Series, Timedelta, Timestamp\n",
    "from plotly import subplots\n",
    "from plotly.graph_objs import Bar, Candlestick, Figure, Scatter\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch import Tensor\n",
    "from torch.nn import LSTM, Linear, Module, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "PLHjpQH6yTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup notebook constants and running environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# project base directory\n",
    "BASE_DIRECTORY = Path(\".\")\n",
    "\n",
    "# detect running environment\n",
    "COLAB_ON = True if \"google.colab\" in str(get_ipython()) else False"
   ],
   "outputs": [],
   "metadata": {
    "id": "82e_w9dyyTjY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set Ploty renderer\n",
    "if COLAB_ON:\n",
    "    pio.renderers.default = \"colab\""
   ],
   "outputs": [],
   "metadata": {
    "id": "fToRyDS0yTjY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and display raw datasets"
   ],
   "metadata": {
    "id": "-kXTLBmBfxuL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load dataset from file system\n",
    "def load_data(\n",
    "    path: Path,\n",
    ") -> DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(ENOENT, os.strerror(ENOENT), path)\n",
    "\n",
    "    table = pq.read_table(path)\n",
    "    return table.to_pandas()\n",
    "\n",
    "\n",
    "# show a dataframe as a table\n",
    "def display_dataframe(\n",
    "        dataframe: DataFrame,\n",
    ") -> None:    \n",
    "    if COLAB_ON:\n",
    "        spec = importlib.util.find_spec(\"google.colab\")\n",
    "        if spec:            \n",
    "            data_table = importlib.import_module(\"google.colab.data_table\")            \n",
    "            enable_dataframe_formatter = getattr(\n",
    "                data_table, \n",
    "                \"enable_dataframe_formatter\",\n",
    "            )            \n",
    "            \n",
    "            enable_dataframe_formatter()            \n",
    "           \n",
    "    display.display(dataframe[:20000] if COLAB_ON else dataframe) \n",
    "  \n",
    "  \n",
    " #show a timeseries graph of a selected attribute\n",
    "def plot_dataset(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    scatter = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[column],\n",
    "        mode=\"lines\",\n",
    "        name=\"values\",\n",
    "    )\n",
    "\n",
    "    figure.add_trace(scatter)\n",
    "    figure.update_layout(\n",
    "        showlegend=False,\n",
    "        title_text=column,\n",
    "        template=\"simple_white\",\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(showgrid=True)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "iUo9bi14yTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# buses dataset\n",
    "buses_filename = \"buses.parquet\"\n",
    "buses_path = BASE_DIRECTORY.joinpath(buses_filename)\n",
    "buses_dataset = load_data(buses_path)\n",
    "\n",
    "\n",
    "# classes dataset\n",
    "classes_filename = \"classes.parquet\"\n",
    "classes_path = BASE_DIRECTORY.joinpath(classes_filename)\n",
    "classes_dataset = load_data(classes_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 (SOLUTION): Sound protection with differential privacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fourier Perturbation Algorithm (FPA)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Perturb timeline series with differential privacy\n",
    "def fpa(Q: ndarray, δ: float, ε: float, k: int) -> ndarray:\n",
    "\n",
    "    # define a laplace mechanism for perturbation\n",
    "    def lpa(Q: ndarray, δ: float, ε: float) -> ndarray:\n",
    "        # differential privacy scale based on the budget\n",
    "        λ = δ / ε\n",
    "\n",
    "        # Laplace mechanism applied to whole serie\n",
    "        Z = np.random.laplace(scale=λ, size=Q.size)\n",
    "\n",
    "        return Q + Z\n",
    "\n",
    "    # discrete Fourier trasform\n",
    "    F = np.fft.fft(Q)\n",
    "\n",
    "    # first k values of DFT\n",
    "    F_k = F[:k]\n",
    "\n",
    "    # lpa of F_k\n",
    "    Fλ_k = lpa(F_k.real, δ, ε) + 1j * lpa(F_k.imag, δ, ε)\n",
    "\n",
    "    # Fλ_k with `n - k` zero-padding\n",
    "    Fλ_n = np.pad(Fλ_k, (0, Q.size - k))\n",
    "\n",
    "    # inverse discrete Fourier transform\n",
    "    Qλ = np.fft.ifft(Fλ_n)\n",
    "\n",
    "    # modulus of complex values of IFFT\n",
    "    Qλ_m = np.absolute(Qλ)\n",
    "\n",
    "    # round perturbation to integers\n",
    "    Qλ_int = np.rint(Qλ_m)\n",
    "\n",
    "    # replace negative values with zeroes\n",
    "    Qλ_int[Qλ_int < 0] = 0\n",
    "\n",
    "    return Qλ_int\n",
    "\n",
    "\n",
    "# perform a noise perturbation with the Rastogi algorithm\n",
    "def fourier_perturbation(\n",
    "    sequence: Series,\n",
    "    boundary: float,\n",
    "    budget: float,\n",
    "    coefficients: int,\n",
    ") -> Optional[ndarray]:\n",
    "\n",
    "    # calculate the L-norm of a uniform vector of seed values\n",
    "    def norm(seed: float, size: int, order: int) -> float:\n",
    "        serie = np.full((size,), seed)\n",
    "        return linalg.norm(serie, order)\n",
    "\n",
    "    size = sequence.size\n",
    "    if size  > coefficients:\n",
    "        sensitivity = math.sqrt(coefficients) * norm(boundary, size, 2)\n",
    "        return fpa(\n",
    "            sequence.to_numpy(),\n",
    "            sensitivity,\n",
    "            budget,\n",
    "            coefficients,\n",
    "        )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def bound(\n",
    "    serie: Series,\n",
    "    aggregate: str,\n",
    ") -> float:\n",
    "    def ceil(serie: Series) -> float:\n",
    "        maximum = serie.max()\n",
    "        # maximum = linalg.norm(Q, np.inf)\n",
    "        # # round(maximum, -1)\n",
    "        return 10 * math.ceil(maximum / 10)\n",
    "\n",
    "    return {\n",
    "        \"count\": 1,\n",
    "        \"sum\": ceil(serie),\n",
    "    }.get(aggregate, NA)\n",
    "\n",
    "\n",
    "def get_fourier_perturbations(\n",
    "    dataframe: DataFrame,\n",
    "    agg_sizes: Sequence[int],\n",
    "    coefficients: Sequence[int],\n",
    "    epsilons: Sequence[float],\n",
    "    stops: Optional[Sequence[str]],\n",
    ") -> DataFrame:    \n",
    "    dataframe_ = dataframe.copy()\n",
    "    if stops:\n",
    "        dataframe_ = dataframe_[dataframe_[\"stop_name\"].isin(stops)]\n",
    "\n",
    "    # count validations by bus stop (per user and timestamp)\n",
    "    dataframe_ = (\n",
    "        dataframe_.groupby([\"id\", \"departure_time\"])\n",
    "        .count()[\"stop_id\"]\n",
    "        .to_frame(name=\"validation\")\n",
    "        .reset_index()        \n",
    "    )\n",
    "\n",
    "    samples = DataFrame()\n",
    "    for n in agg_sizes:\n",
    "        subset = dataframe_[\"id\"].drop_duplicates().sample(n).values\n",
    "        mask = dataframe_[\"id\"].isin(subset)\n",
    "        sample = dataframe_[mask].reset_index(drop=True)\n",
    "        sample = sample.assign(n=n).drop(\"id\", axis=1)\n",
    "        samples = samples.append(sample)\n",
    "\n",
    "    fpas = DataFrame()\n",
    "    for n in agg_sizes:\n",
    "        sample = samples.query(f\"n=={n}\")\n",
    "        reference = sample.groupby(\"departure_time\").aggregate(\"count\")\n",
    "        boundary = bound(sample[\"validation\"], \"count\")\n",
    "        for k, ε in itertools.product(coefficients, epsilons):\n",
    "            iteration = reference.copy()\n",
    "            iteration = iteration.assign(n=n, ε=ε, k=k)\n",
    "            # iteration[\"fpa\"] = fourier_perturbation(\n",
    "            #     iteration[\"validation\"],\n",
    "            #     boundary,\n",
    "            #     ε,\n",
    "            #     k,\n",
    "            # )\n",
    "            \n",
    "            iteration[\"fpa\"] = weekly_fpa(\n",
    "                iteration[\"validation\"],\n",
    "                boundary,\n",
    "                ε,\n",
    "                k,\n",
    "            )\n",
    "\n",
    "            iteration[\"noise\"] = iteration[\"fpa\"] - iteration[\"validation\"]\n",
    "            fpas = fpas.append(iteration)\n",
    "\n",
    "    return fpas\n",
    "\n",
    "def weekly_fpa(\n",
    "    sequence: Series,\n",
    "    boundary: float, \n",
    "    epsilon: float,\n",
    "    coefficients: int,\n",
    ") -> Series:\n",
    "    dataframe = sequence.copy().to_frame()    \n",
    "    dataframe[\"week\"] = dataframe.index.isocalendar().week    \n",
    "    weeks = dataframe[\"week\"].unique()    \n",
    "    weekly_fpas = []\n",
    "    for week in weeks:\n",
    "        weekly_sequence = dataframe[dataframe[\"week\"] == week]        \n",
    "        weekly_fpa = fourier_perturbation(\n",
    "                weekly_sequence[\"validation\"],\n",
    "                boundary,\n",
    "                epsilon,\n",
    "                coefficients,\n",
    "            )\n",
    "        \n",
    "        weekly_fpas.append(weekly_fpa)\n",
    "        \n",
    "    dataframe[\"fpa\"] = np.concatenate(weekly_fpas).ravel()\n",
    "    return dataframe[\"fpa\"]\n",
    "        \n",
    "\n",
    "def facet_plot(\n",
    "    dataframe: DataFrame,\n",
    "    size: int,\n",
    "    row: str,\n",
    "    col: str,\n",
    ") -> None:\n",
    "    dataset = dataframe.query(f\"n=={size}\").reset_index()\n",
    "    figure = px.line(\n",
    "        dataset,\n",
    "        x=\"departure_time\",\n",
    "        y=\"fpa\",\n",
    "        facet_row=row,\n",
    "        facet_col=col,\n",
    "        labels = {'departure_time': '', 'fpa': ''},\n",
    "        #facet_row_spacing=0.01,\n",
    "        #facet_col_spacing=0.01,\n",
    "    )\n",
    "                                                                                                                                    \n",
    "    figure.update_yaxes(matches=None, showticklabels=False)\n",
    "    figure.update_xaxes(showticklabels=False)\n",
    "    #figure.update_coloraxes(showscale=False)\n",
    "                                                                                                                                                    \n",
    "    trace = Scatter(\n",
    "        x=dataset.departure_time, \n",
    "        y=dataset.validation,\n",
    "        name=\"count\", \n",
    "        line=dict(color=\"gray\", width=0.1, dash=\"dot\"),  \n",
    "        opacity=0.35,\n",
    "    )\n",
    "\n",
    "    trace.update(showlegend=False)\n",
    "    for i, _ in enumerate(dataset[row].unique(), start=1):\n",
    "        for j, _ in enumerate(dataset[col].unique(), start=1):\n",
    "            figure.add_trace(trace, row=i, col=j)\n",
    "\n",
    "    figure.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        title=f\"FPA for n={size}\",\n",
    "        xaxis_title=\"date\",\n",
    "        yaxis_title=\"count\"\n",
    "    )\n",
    "\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# target bus stops\n",
    "beaulieu = [\n",
    "    \"Les Préales\",\n",
    "    \"Tournebride\",\n",
    "    \"Beaulieu Chimie\",\n",
    "    \"Beaulieu INSA\",\n",
    "    \"Beaulieu Restau U\",\n",
    "]\n",
    "\n",
    "# aggregate size\n",
    "Ν = [3500]\n",
    "\n",
    "# Fourier coefficients\n",
    "Κ = [20, 30, 40] ## max len = 3\n",
    "\n",
    "# perturbation budget\n",
    "Ε = [0.05, 0.1, 0.25] ## max len = 3\n",
    "\n",
    "fpas = get_fourier_perturbations(buses_dataset, Ν, Κ, Ε, stops=beaulieu)\n",
    "facet_plot(fpas, 3500, row=\"ε\", col=\"k\")\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# END : Observe\n",
    "####################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training a *safe* neural network\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pre-process raw data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pre processing transportation data\n",
    "def pre_process_by_aggregation(\n",
    "    dataframe: DataFrame,\n",
    "    *,\n",
    "    stops: Optional[Sequence[str]],\n",
    "    ignore_weekend: bool = False,\n",
    ") -> DataFrame:\n",
    "\n",
    "    dataframe_ = dataframe.copy()\n",
    "    # filter data from 'bus_stops' only\n",
    "    if stops:\n",
    "        dataframe_ = dataframe_[dataframe_[\"stop_name\"].isin(stops)]\n",
    "\n",
    "    # remove weekend information\n",
    "    if ignore_weekend:\n",
    "        dataframe_ = dataframe_.set_index(\"departure_time\")\n",
    "        dataframe_ = dataframe_[dataframe_.index.dayofweek < 5]\n",
    "\n",
    "    # aggregate dataset by stop name and departure time\n",
    "    dataframe_ = (\n",
    "        dataframe_.groupby(\n",
    "            [\n",
    "                \"stop_name\",\n",
    "                \"departure_time\",\n",
    "            ]\n",
    "        )\n",
    "        .agg({\"count\": \"sum\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return dataframe_.groupby(\"departure_time\").sum()\n",
    "    \n",
    " \n",
    "def post_processing_by_perturbation(\n",
    "    dataframe: DataFrame,\n",
    "    *,\n",
    "    budget: float,\n",
    "    coefficients: int,\n",
    ") -> DataFrame:\n",
    "    \n",
    "    dataframe_ = dataframe.copy()\n",
    "    boundary = bound(dataframe_[\"count\"], \"count\")\n",
    "    dataframe_.rename({\"count\": \"validation\"}, axis=1, inplace=True)\n",
    "    dataframe_[\"fpa\"] = weekly_fpa(\n",
    "        dataframe_[\"validation\"], \n",
    "        boundary, \n",
    "        budget, \n",
    "        coefficients,\n",
    "    )\n",
    "    \n",
    "    return dataframe_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "####################\n",
    "# BEGIN : Play\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BUDGET = 0.5\n",
    "\n",
    "COEFFICIENTS = 40"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "# BEGIN : Play\n",
    "####################\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aggregated_buses_dataset = pre_process_by_aggregation(\n",
    "    buses_dataset,\n",
    "    stops=beaulieu,\n",
    ")\n",
    "display_dataframe(aggregated_buses_dataset)\n",
    "\n",
    "aggregated_buses_dataset = post_processing_by_perturbation(\n",
    "    aggregated_buses_dataset,\n",
    "    budget=BUDGET, \n",
    "    coefficients=COEFFICIENTS,\n",
    ")\n",
    "\n",
    "plot_dataset(aggregated_buses_dataset, \"fpa\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of validations"
   ],
   "metadata": {
    "id": "FX__8ghegg3F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_dataset(aggregated_buses_dataset, \"validation\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "33PDn8bpyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Number of students"
   ],
   "metadata": {
    "id": "UjmGFdCDgpk3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_dataframe(classes_dataset)\n",
    "plot_dataset(classes_dataset, \"nombre_etudiant\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "OP9gJJueyTk8",
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge datasets\n",
    "def merge_datasets(\n",
    "    classes: DataFrame,\n",
    "    buses: DataFrame,\n",
    ") -> DataFrame:\n",
    "\n",
    "    # ignore dataset entries that are not available in classes timeline\n",
    "    buses_ = buses[\n",
    "        buses.index\n",
    "        <= classes.index.max()\n",
    "        + Timedelta(\n",
    "            1,\n",
    "            unit=\"day\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # merge datasets\n",
    "    dataset = pd.merge(\n",
    "        classes,\n",
    "        buses_,\n",
    "        how=\"outer\",\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # fill empty values\n",
    "    dataset = dataset.fillna(0)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def plot_distributions(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    noise = dataframe_[\"validation\"] - dataframe_[\"fpa\"]\n",
    "    # noise += np.abs(noise.min())\n",
    "    figure = ff.create_distplot(\n",
    "        [noise, dataframe_[\"validation\"], dataframe_[\"fpa\"]], \n",
    "        ['noise', \"validations\", \"fpa\"],\n",
    "        curve_type='normal',\n",
    "        bin_size=[3, 3, 3], \n",
    "    )\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "8eOCLUJEyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display the merged dataset (students buses, and perturbed buses)"
   ],
   "metadata": {
    "id": "4kpiAoimhBJ6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = merge_datasets(classes_dataset, aggregated_buses_dataset)\n",
    "display_dataframe(dataset)\n",
    "\n",
    "\n",
    "print(\"Distributions of datasets\")\n",
    "plot_distributions(dataset)"
   ],
   "outputs": [],
   "metadata": {
    "id": "ZMIuDquXyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data preparation for the neural network"
   ],
   "metadata": {
    "id": "F-CFufprhULZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add features (motifs) to the dataset\n",
    "la_rentree = Timestamp(\"2021-09-06\")\n",
    "la_toussaint = Timestamp(\"2021-11-01\")\n",
    "one_week_timedelta = Timedelta(7, unit=\"day\")\n",
    "\n",
    "# bucketize attribute\n",
    "def onehot_encode(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    ") -> DataFrame:\n",
    "    dummies = pd.get_dummies(\n",
    "        dataframe[column],\n",
    "        prefix=column,\n",
    "    )\n",
    "\n",
    "    return pd.concat(\n",
    "        [dataframe, dummies],\n",
    "        axis=1,\n",
    "    ).drop(columns=[column])\n",
    "\n",
    "\n",
    "# encode (time) column as periodic wave\n",
    "def periodic_encode(\n",
    "    dataframe: DataFrame,\n",
    "    column: str,\n",
    "    period: int,\n",
    "    start_num: int = 0,\n",
    ") -> DataFrame:\n",
    "    kwargs = {\n",
    "        f\"sin_{column}\": lambda x: np.sin(\n",
    "            2 * np.pi * (dataframe[column] - start_num) / period\n",
    "        ),\n",
    "        f\"cos_{column}\": lambda x: np.cos(\n",
    "            2 * np.pi * (dataframe[column] - start_num) / period\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return dataframe.assign(**kwargs).drop(columns=[column])\n",
    "\n",
    "\n",
    "# add uniform timeindex column\n",
    "def set_time_index(\n",
    "    dataframe: DataFrame,\n",
    "    frequence: int = 15,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "\n",
    "    # add a time index using the frequency\n",
    "    dataframe_[\"time_idx\"] = dataframe_.index - dataframe_.index.min()\n",
    "    dataframe_[\"time_idx\"] = (\n",
    "        dataframe_[\"time_idx\"].astype(\"timedelta64[m]\") // frequence\n",
    "    )\n",
    "    dataframe_[\"time_idx\"] = dataframe_[\"time_idx\"].astype(\"int_\")\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "# mark dataset ranges as holidays\n",
    "def label_holidays(\n",
    "    dataframe: DataFrame,\n",
    "    start: Timestamp,\n",
    "    end: Timestamp,\n",
    "    column=\"holiday\",\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    dataframe_[column] = 0\n",
    "    dataframe_.loc[\n",
    "        (dataframe_.index >= start) & (dataframe_.index < end),\n",
    "        column,\n",
    "    ] = 1\n",
    "    return dataframe_\n",
    "\n",
    "\n",
    "# generate lags (to track interaction throughout time)\n",
    "def generate_lags(\n",
    "    dataframe: DataFrame,\n",
    "    lags: int,\n",
    "    column: str,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    for n in range(1, lags + 1):\n",
    "        dataframe_[f\"{column}_lag_{n}\"] = dataframe_[column].shift(n)\n",
    "\n",
    "    return dataframe_.fillna(0)\n",
    "\n",
    "\n",
    "# add features to the dataset\n",
    "def add_features(\n",
    "    dataframe: DataFrame,\n",
    "    bucketize_date: bool = True,\n",
    "    periodic_time: bool = True,\n",
    "    holidays: bool = False,\n",
    "    timeindex: bool = False,\n",
    "    lags: bool = False,\n",
    "    n_lags: int = 50,\n",
    ") -> DataFrame:\n",
    "    dataframe_ = dataframe.copy()\n",
    "    if timeindex:\n",
    "        dataframe_ = set_time_index(dataframe_)\n",
    "\n",
    "    if bucketize_date:\n",
    "        dataframe_ = dataframe_.assign(dayofweek=dataframe_.index.dayofweek)\n",
    "        # .assign(day=dataframe.index.day)\n",
    "        # .assign(month=dataset.index.month)\n",
    "        dataframe_ = onehot_encode(dataframe_, \"dayofweek\")\n",
    "        # dataset = onehot_encode(dataset, \"month\")\n",
    "\n",
    "    if periodic_time:\n",
    "        dataframe_ = dataframe_.assign(hour=dataframe_.index.hour)\n",
    "        dataframe_ = dataframe_.assign(minute=dataframe_.index.minute)\n",
    "        dataframe_ = periodic_encode(dataframe_, \"hour\", 24, 0)\n",
    "        dataframe_ = periodic_encode(dataframe_, \"minute\", 60, 0)\n",
    "\n",
    "    if holidays:\n",
    "        dataframe_ = label_holidays(\n",
    "            dataframe_,\n",
    "            la_toussaint,\n",
    "            la_toussaint + one_week_timedelta,\n",
    "        )\n",
    "\n",
    "    if lags:\n",
    "        dataframe_ = generate_lags(dataframe_, n_lags, \"count\")\n",
    "        dataframe_ = generate_lags(dataframe_, n_lags, \"nombre_etudiant\")\n",
    "\n",
    "    # dataframe.drop([\"nombre_etudiant\"], axis=1, inplace=True)\n",
    "    return dataframe_"
   ],
   "outputs": [],
   "metadata": {
    "id": "CXl1HRlCyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = add_features(dataset, holidays=True)"
   ],
   "outputs": [],
   "metadata": {
    "id": "hLSZvIr4yTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Split datasets to train a machine learning tool"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Split the data into test, validation, and train sets\n",
    "def features_split(\n",
    "    dataframe: DataFrame,\n",
    "    target: str,\n",
    ") -> Tuple[DataFrame, DataFrame]:\n",
    "    y = dataframe[[target]]\n",
    "    X = dataframe.drop(columns=[target])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_timestamp_bound(\n",
    "    dataframe: DataFrame,\n",
    "    weeks: int,\n",
    ") -> Timestamp:\n",
    "    timedelta = Timedelta(7 * weeks - 1, unit=\"day\")\n",
    "    timestamp = dataframe.index.min() + timedelta\n",
    "    return timestamp.normalize()"
   ],
   "outputs": [],
   "metadata": {
    "id": "-FGZj9TyyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define the neural network\n"
   ],
   "metadata": {
    "id": "wxRu24uH8vQM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define and run a RNN model\n",
    "class LSTMModel(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            layer_dim,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(\n",
    "            self.layer_dim,\n",
    "            x.size(0),\n",
    "            self.hidden_dim,\n",
    "        ).requires_grad_()\n",
    "\n",
    "        # initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(\n",
    "            self.layer_dim,\n",
    "            x.size(0),\n",
    "            self.hidden_dim,\n",
    "        ).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        # (squeezing is equivalent to: `out = out[:, -1, :]`)\n",
    "        out = torch.squeeze(out)\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {
    "id": "2wjqVABVyTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Configure the neural network"
   ],
   "metadata": {
    "id": "gX7rcuEslD4T"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "HIDDEN_DIM = 64\n",
    "LAYER_DIM = 3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ],
   "outputs": [],
   "metadata": {
    "id": "2mgESKb_yTk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train the neural network"
   ],
   "metadata": {
    "id": "UORhj0JelLX3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Helper to train the NN model\n",
    "class RunnerHelper:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "\n",
    "        # set model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # make predictions\n",
    "        ŷ = self.model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(ŷ, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # reset to zero gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # returns loss\n",
    "        return loss.item()\n",
    "\n",
    "    def val_step(self, X, y):\n",
    "\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # make prediction\n",
    "        ŷ = self.model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(ŷ, y)\n",
    "\n",
    "        # return loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, n_epochs=50):\n",
    "        model_path = f'{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_train_losses = []\n",
    "            for x_train, y_train in train_loader:\n",
    "                # x_train = x_train.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                x_train = torch.unsqueeze(x_train, 1)\n",
    "                train_loss = self.train_step(x_train, y_train)\n",
    "                batch_train_losses.append(train_loss)\n",
    "\n",
    "            training_loss = np.mean(batch_train_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    # x_val = x_val.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                    x_val = torch.unsqueeze(x_val, 1)\n",
    "                    val_loss = self.val_step(x_val, y_val)\n",
    "                    batch_val_losses.append(val_loss)\n",
    "\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 20 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch:3d}/{n_epochs}] Training loss: {training_loss:.4f}\"\n",
    "                    f\"\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        # torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                # x_test = x_test.view([batch_size, -1, n_features]).to(DEVICE)\n",
    "                x_test = torch.unsqueeze(x_test, 1)\n",
    "                self.model.eval()\n",
    "                ŷ = self.model(x_test)\n",
    "                predictions.append(ŷ.detach().numpy())\n",
    "                values.append(y_test.detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        figure = Figure()\n",
    "        tics = [*range(len(self.train_losses) + 1)]\n",
    "        value = Scatter(\n",
    "            x=tics,\n",
    "            y=self.train_losses,\n",
    "            mode=\"lines\",\n",
    "            name=\"Training\",\n",
    "            marker=dict(),\n",
    "        )\n",
    "\n",
    "        figure.add_trace(value)\n",
    "        value = Scatter(\n",
    "            x=tics,\n",
    "            y=self.val_losses,\n",
    "            mode=\"lines\",\n",
    "            name=\"Validation\",\n",
    "            marker=dict(),\n",
    "        )\n",
    "\n",
    "        figure.add_trace(value)\n",
    "        figure.update_layout(title_text=\"Losses\")\n",
    "        figure.update_xaxes(title_text=\"epoch\")\n",
    "        figure.update_yaxes(title_text=\"loss (%)\")\n",
    "        figure.show()\n",
    "\n",
    "\n",
    "# rescale results and align it to original time index\n",
    "def inverse_transform(\n",
    "    values: Sequence[ndarray],\n",
    "    predictions: Sequence[ndarray],\n",
    "    index: DatetimeIndex,\n",
    "    scaler: MinMaxScaler,\n",
    ") -> DataFrame:\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    dataframe = DataFrame(\n",
    "        data={\n",
    "            \"value\": vals,\n",
    "            \"prediction\": preds,\n",
    "        },\n",
    "        index=index[: len(vals)],\n",
    "    )\n",
    "\n",
    "    dataframe = dataframe.sort_index()\n",
    "    dataframe = DataFrame(\n",
    "        scaler.inverse_transform(dataframe),\n",
    "        columns=dataframe.columns,\n",
    "        index=dataframe.index,\n",
    "    )\n",
    "\n",
    "    return dataframe.astype(\"int_\")\n",
    "\n",
    "\n",
    "# formating data for NN\n",
    "def to_dataloaders(\n",
    "    dataframe_train: Tuple[DataFrame, DataFrame],\n",
    "    dataframe_val: Tuple[DataFrame, DataFrame],\n",
    "    dataframe_test: Tuple[DataFrame, DataFrame],\n",
    "    scaler: MinMaxScaler,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    # scale data\n",
    "    X_train_arr = scaler.fit_transform(dataframe_train[0])\n",
    "    X_val_arr = scaler.transform(dataframe_val[0])\n",
    "    X_test_arr = scaler.transform(dataframe_test[0])\n",
    "\n",
    "    y_train_arr = scaler.fit_transform(dataframe_train[1])\n",
    "    y_val_arr = scaler.transform(dataframe_val[1])\n",
    "    y_test_arr = scaler.transform(dataframe_test[1])\n",
    "\n",
    "    # transform scaled data to tensors\n",
    "    train_features = Tensor(X_train_arr)\n",
    "    train_targets = Tensor(y_train_arr)\n",
    "    val_features = Tensor(X_val_arr)\n",
    "    val_targets = Tensor(y_val_arr)\n",
    "    test_features = Tensor(X_test_arr)\n",
    "    test_targets = Tensor(y_test_arr)\n",
    "\n",
    "    # setup tensor datasets\n",
    "    train = TensorDataset(train_features, train_targets)\n",
    "    val = TensorDataset(val_features, val_targets)\n",
    "    test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    # setup (tensor) datasets loaders\n",
    "    train_loader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test,\n",
    "        batch_size=1,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_ = dataset.drop(labels=[\"validation\"], axis=1)\n",
    "end_train = get_timestamp_bound(dataset_, weeks=9)\n",
    "end_val = get_timestamp_bound(dataset_, weeks=10)\n",
    "\n",
    "train_dataset = dataset_[dataset_.index < end_train]\n",
    "val_dataset = dataset_[(dataset_.index >= end_train) & (dataset_.index < end_val)]\n",
    "test_dataset = dataset_[dataset_.index >= end_val]\n",
    "\n",
    "X_train, y_train = features_split(train_dataset, target=\"fpa\")\n",
    "X_val, y_val = features_split(val_dataset, target=\"fpa\")\n",
    "X_test, y_test = features_split(test_dataset, target=\"fpa\")\n",
    "\n",
    "input_dim = len(X_train.columns) \n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    layer_dim=LAYER_DIM,\n",
    "    output_dim=1,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "loss_fn = MSELoss() \n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "runner = RunnerHelper(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "train_loader, val_loader, test_loader = to_dataloaders(\n",
    "    (X_train, y_train),\n",
    "    (X_val, y_val),\n",
    "    (X_test, y_test),    \n",
    "    scaler,\n",
    "    BATCH_SIZE,\n",
    ")\n",
    "\n",
    "runner.train(train_loader, val_loader, n_epochs=EPOCHS)\n",
    "runner.plot_losses()\n",
    "predictions, values = runner.evaluate(test_loader)\n",
    "fpa_result = inverse_transform(values, predictions, X_test.index, scaler)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Xr5aNd4VyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_residuals(\n",
    "    dataframe: DataFrame,\n",
    ") -> None:\n",
    "    hovertext = []\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        hovertext.append(\n",
    "            f\"{dataframe.index[i]}<br>\"\n",
    "            f\"Real: {dataframe['value'][i]}<br>\"\n",
    "            f\"Prediction: {dataframe['prediction'][i]}\"\n",
    "        )\n",
    "\n",
    "    figure = Figure(\n",
    "        data=[\n",
    "            Scatter(\n",
    "                x=dataframe.index,\n",
    "                y=dataframe[\"value\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"reference\",\n",
    "                line=dict(color=\"lightgrey\", width=0.6, dash=\"dot\"),\n",
    "                # opacity=0.6,\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            Scatter(\n",
    "                x=dataframe.index,\n",
    "                y=dataframe[\"prediction\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"prediction\",\n",
    "                line=dict(color=\"lightblue\", width=0.6, dash=\"dot\"),\n",
    "                showlegend=False,\n",
    "                # opacity=0.6,\n",
    "            ),\n",
    "            Candlestick(\n",
    "                x=dataframe.index,\n",
    "                open=dataframe[\"value\"],\n",
    "                high=dataframe[\"prediction\"],\n",
    "                low=dataframe[\"prediction\"],\n",
    "                close=dataframe[\"value\"],\n",
    "                text=hovertext,\n",
    "                hoverinfo=\"text\",\n",
    "                name=\"residuals\",\n",
    "                # line=dict(width=2),\n",
    "                increasing_line_color=\"lightseagreen\",\n",
    "                decreasing_line_color=\"lightsalmon\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    figure.update_layout(\n",
    "        title=\"Prediction residuals\",\n",
    "        template=\"simple_white\",\n",
    "        xaxis_rangeslider_visible=True,\n",
    "    )\n",
    "\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comparing the neural network against with original dataset"
   ],
   "metadata": {
    "id": "EaQUkM6X8_2s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_ = dataset.drop(labels=[\"fpa\"], axis=1)\n",
    "end_train = get_timestamp_bound(dataset_, weeks=9)\n",
    "end_val = get_timestamp_bound(dataset_, weeks=10)\n",
    "\n",
    "train_dataset = dataset_[dataset_.index < end_train]\n",
    "val_dataset = dataset_[(dataset_.index >= end_train) & (dataset_.index < end_val)]\n",
    "test_dataset = dataset_[dataset_.index >= end_val]\n",
    "\n",
    "X_train, y_train = features_split(train_dataset, target=\"validation\")\n",
    "X_val, y_val = features_split(val_dataset, target=\"validation\")\n",
    "X_test, y_test = features_split(test_dataset, target=\"validation\")\n",
    "\n",
    "input_dim = len(X_train.columns) \n",
    "model = LSTMModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    layer_dim=LAYER_DIM,\n",
    "    output_dim=1,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "loss_fn = MSELoss() \n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "runner = RunnerHelper(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "train_loader, val_loader, test_loader = to_dataloaders(\n",
    "    (X_train, y_train),\n",
    "    (X_val, y_val),\n",
    "    (X_test, y_test),    \n",
    "    scaler,\n",
    "    BATCH_SIZE,\n",
    ")\n",
    "\n",
    "runner.train(train_loader, val_loader, n_epochs=EPOCHS)\n",
    "runner.plot_losses()\n",
    "predictions, values = runner.evaluate(test_loader)\n",
    "lstm_result = inverse_transform(values, predictions, X_test.index, scaler)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Visualize the predictions of the two models"
   ],
   "metadata": {
    "id": "k8nCaMtem6sC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_models_prediction_interval(\n",
    "    dataframe: DataFrame,\n",
    "    rnn_dataframe: DataFrame,\n",
    "    baseline_dataframe: DataFrame,\n",
    ") -> None:\n",
    "    figure = Figure()\n",
    "    value = Scatter(\n",
    "        x=dataframe.index,\n",
    "        y=dataframe[\"validation\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Reference\",        \n",
    "        line=dict(color=\"rgba(0,0,0, 0.3)\", width=1, dash=\"dot\"),\n",
    "    )\n",
    "\n",
    "    figure.add_trace(value)\n",
    "    baseline = Scatter(\n",
    "        x=baseline_dataframe.index,\n",
    "        y=baseline_dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"Standard forecasting\",\n",
    "        visible=\"legendonly\",\n",
    "        opacity=0.8,\n",
    "    )\n",
    "\n",
    "    figure.add_trace(baseline)\n",
    "    prediction = Scatter(\n",
    "        x=rnn_dataframe.index,\n",
    "        y=rnn_dataframe.prediction,\n",
    "        mode=\"lines\",\n",
    "        name=\"FPA forecasting\",        \n",
    "        opacity=0.8,        \n",
    "    )\n",
    "\n",
    "    figure.add_trace(prediction)\n",
    "    figure.update_layout(\n",
    "        showlegend=True,\n",
    "        title_text=\"Predictions\",\n",
    "        template=\"simple_white\",\n",
    "        xaxis=dict(\n",
    "            range=[\n",
    "                rnn_dataframe.index.min(),\n",
    "                rnn_dataframe.index.max(),\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    figure.update_xaxes(rangeslider_visible=True)\n",
    "    figure.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "pNEy9gZNyTk8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_models_prediction_interval(dataset, fpa_result, lstm_result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\n",
    "- Vibhor Rastogi and Suman Nath. Differentially private aggregation of distributed time-series with transformation and encryption. Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, June 2010, Indianapolis (IN) USA [[DOI]](https://doi.org/10.1145/1807167.1807247)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NmKSCNfpfOQ-",
    "qzLkzFP3fZm-",
    "O4teYxHrfnuh",
    "-kXTLBmBfxuL",
    "ZV4MJqgogTB0",
    "F-CFufprhULZ",
    "b5Hiwx5O7gXw",
    "tawuGE-n9PTw",
    "s4gBkGhqpaL7"
   ],
   "include_colab_link": true,
   "name": "a2r2-notebook03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "d5511370cecfc9f72087461b207d0bd90f18099e89758b2a61eb3e3243f66294"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.4 64-bit ('venv-a2r2': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}