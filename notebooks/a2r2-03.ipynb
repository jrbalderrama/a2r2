{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrbalderrama/a2r2/blob/main/notebooks/a2r2-03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qOVUmdqyTjY"
   },
   "source": [
    "# RUDI Workshop: Introduction to Privacy-Preserving Data Publishing Techniques\n",
    "\n",
    "Tristan ALLARD & Javier ROJAS BALDERRAMA\n",
    "\n",
    "_Univ Rennes, CNRS, INRIA_\n",
    "  \n",
    "This work is licensed under a [Creative Commons Zero v1.0 Universal License](https://creativecommons.org/publicdomain/zero/1.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "We warmly thank François Bodin and Luc Lesoil for their support on the data and the definition of the use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook __THREE__: Protection with differential privacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 (STARTER) \n",
    "This notebook introduces you to the fashionable protection measure called _differential privacy_ ! Similarly to the first notebook, you will have to answer to the question asked by the use case: does a change in the students schedules at the Beaulieu campus impact the load of the buses that go through the campus ? However, motivated by the reidentification attacks that you performed in the second notebook, you will be much more cautious now with privacy. Prior to performing your analysis, you will perturb the buses validation frequencies such that differential privacy is satisfied. You will then launch your analysis on the perturbed dataset. This allows you to observe and manipulate a differentially private perturbation algorithm, although, of course, in real life, the analyst only receives the already perturbed dataset to be used for its analysis : the perturbation is obviously performed before the data is exported. \n",
    "\n",
    "You can now go to [Step 1](#step_1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmKSCNfpfOQ-"
   },
   "source": [
    "## Settings and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzLkzFP3fZm-"
   },
   "source": [
    "\n",
    " ### Download datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_Eju0G4yTjY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget -nv -nc https://zenodo.org/record/5509268/files/buses.parquet\n",
    "!wget -nv -nc https://zenodo.org/record/5519319/files/classes.parquet -O classes_filiere.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4teYxHrfnuh"
   },
   "source": [
    " ### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLHjpQH6yTjY"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from atelier import data, io, plot\n",
    "from atelier.extensions.star import preprocessing as spg\n",
    "from atelier.extensions.star import privacy as spy\n",
    "from atelier.extensions.star.preprocessing import FourierPerturbationTransformer\n",
    "from atelier.learn import model, preprocessing\n",
    "from atelier.plot import timeline\n",
    "from atelier.utils import time\n",
    "from IPython.display import display\n",
    "from pandas import Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup notebook constants and running environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82e_w9dyyTjY"
   },
   "outputs": [],
   "source": [
    "from atelier.utils import colaboratory\n",
    "\n",
    "# check running environment\n",
    "COLAB_ON = colaboratory.setup()\n",
    "\n",
    "RANDOM_STATE = None\n",
    "\n",
    "PERIOD = \"week\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kXTLBmBfxuL"
   },
   "source": [
    "### Load and display raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buses dataset\n",
    "buses_path = Path(\"buses.parquet\")\n",
    "buses_dataset = io.read_data(buses_path)\n",
    "\n",
    "# classes dataset\n",
    "classes_path = Path(\"classes_filiere.parquet\")\n",
    "classes_dataset = io.read_data(classes_path).rename(\n",
    "    columns={\n",
    "        \"nombre_etudiant\": \"students\",\n",
    "        \"filiere\": \"background\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 (SOLUTION): Sound protection with differential privacy\n",
    "<a id='step_1'></a>\n",
    "\n",
    "A simple way to satisfy differential privacy when exporting the buses validation datasets could be first to aggregate the buses validations (i.e., count the number of validations at each timestamp) and second to add a random value to each count sampled in a well parameterized Laplace distribution. However, the longer the time series, the bigger the noise. In our use case, the perturbation would be probably bigger than the counts. So we use the _FPA algorithm_, an algorithm that has been specifically designed for perturbing long time series. It implements a simple idea: transform the time series to its Fourier representation, perturb the first _k_ coefficients and discard the others, and transform back the coefficients to the initial representation. \n",
    "\n",
    "You can have a look at our implementation of the FPA algorithm but this is not mandatory. In order to progress, you only need to know that you can tune two parameters : _k_ and _ε_. \n",
    "- The higher the value of _k_, the more precise the Fourier representation but the higher the noise. You can think of _k_ as being usually on the order of 20. \n",
    "- The higher the value of _ε_, the lower the noise (so the lower the privacy guarantees). You can think of _ε_ as being usually on the order of 0.1 . \n",
    "\n",
    "You can now observe the time series resulting from the use of different values for these parameters [here](#observeexampledp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Perturbation Algorithm (FPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target bus stops\n",
    "beaulieu_stops = [\n",
    "    \"Les Préales\",\n",
    "    \"Tournebride\",\n",
    "    \"Beaulieu Chimie\",\n",
    "    \"Beaulieu INSA\",\n",
    "    \"Beaulieu Restau U\",\n",
    "]\n",
    "\n",
    "# aggregate size\n",
    "Ν = [3500]\n",
    "\n",
    "# Fourier coefficients\n",
    "Κ = [30, 35, 40]  ## max len = 3\n",
    "\n",
    "# perturbation budget\n",
    "Ε = [0.01, 0.05, 0.1, 0.15, 0.20]  # week ## max len = 3\n",
    "# Ε = [0.01, 0.05, 0.1, 0.25, 0.5]  # day ## max len = 3\n",
    "fpas = spy.get_fourier_perturbations(\n",
    "    buses_dataset,\n",
    "    Ν,\n",
    "    Κ,\n",
    "    Ε,\n",
    "    attribute=\"stop_name\",\n",
    "    value=beaulieu_stops,\n",
    "    period=PERIOD,\n",
    "    random_state=RANDOM_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='observeexampledp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.facet_plot(fpas, 3500, row=\"ε\", col=\"k\")\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END : Observe\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Food for thoughts__ \n",
    "\n",
    "```\n",
    "####################\n",
    "# BEGIN : Answer\n",
    "```\n",
    "\n",
    "- Which couple of parameters seem to perturb the least the original time series ? \n",
    "\n",
    "```\n",
    "# END : Answer\n",
    "####################\n",
    "```\n",
    "\n",
    "You can now [play with these parameters](#playwithdp) in order to observe the impact of the FPA algorithm on the time series representing the number of validations at each timestamp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a *safe* neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='playwithdp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# BEGIN : Play\n",
    "\n",
    "BUDGET = 0.1\n",
    "\n",
    "COEFFICIENTS = 40\n",
    "\n",
    "# END : Play\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can [observe here](#observedp) the resulting time series and [compare here](#observedistributionsdp) the distributions of the original counts to the distribution of the perturbed counts\n",
    "\n",
    "Hum... Do you want to observe the resulting predictions ? Sure ? \n",
    "Well, you can go to [Step 2](#step_2) and come back later for trying other parameters ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "buses_dataset = data.aggregate_dataframe(\n",
    "    buses_dataset,\n",
    "    by=\"departure_time\",\n",
    "    agg={\"count\": \"sum\"},\n",
    "    attribute=\"stop_name\",\n",
    "    value=beaulieu_stops,\n",
    ").rename(columns={\"count\": \"validations\"})\n",
    "display(buses_dataset)\n",
    "\n",
    "protected_buses_dataset = buses_dataset.copy()\n",
    "transformer = FourierPerturbationTransformer(\n",
    "    BUDGET,\n",
    "    COEFFICIENTS,\n",
    "    PERIOD,\n",
    "    RANDOM_STATE,\n",
    ")\n",
    "\n",
    "protected_buses_dataset[\"fpa\"] = transformer.fit_transform(\n",
    "    protected_buses_dataset[\"validations\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='observedp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# BEGIN : Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline.plot(protected_buses_dataset, \"fpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END : Observe\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go back to [playing with the parameters](#playwithdp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kpiAoimhBJ6"
   },
   "source": [
    "#### Display the merged dataset (students buses, and perturbed buses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMIuDquXyTk8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = data.merge_dataframes(classes_dataset, protected_buses_dataset)\n",
    "display(dataset)\n",
    "\n",
    "timeline.plot_with_annotations(\n",
    "    dataset,\n",
    "    attributes=[\"validations\", \"fpa\"],\n",
    "    title=\"Count of Students & Validations\",\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='observedistributionsdp'></a>\n",
    "\n",
    "You can observe below the distributions of (1) the original counts of buses validations (_validations_), (2) the perturbed counts of buses validations (_fpa_), and (3) the distribution of the differences between the perturbed counts and the original counts (_noise_). \n",
    "\n",
    "When done, you can go back to [playing with the parameters](#playwithdp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# BEGIN : Observe\n",
    "\n",
    "print(\"Distributions of datasets\")\n",
    "plot.distributions_plot(\n",
    "    dataset,\n",
    "    attributes=[\"validations\", \"fpa\"],\n",
    "    curve_type=\"kde\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END: Observe\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go back to [playing with the parameters](#playwithdp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-CFufprhULZ"
   },
   "source": [
    "#### Data preparation for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXl1HRlCyTk8"
   },
   "outputs": [],
   "source": [
    "# Add features (motifs) to the dataset\n",
    "la_toussaint = Timestamp(\"2021-11-01\")\n",
    "\n",
    "start_test = time.get_next_monday(dataset, weeks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLSZvIr4yTk8"
   },
   "outputs": [],
   "source": [
    "# swap col. names to use the column_transformer of timeline_feat_extraction\n",
    "dataset[[\"validations\", \"fpa\"]] = dataset[[\"fpa\", \"validations\"]]\n",
    "\n",
    "holidays = pd.date_range(start=la_toussaint, periods=7, freq=\"D\")\n",
    "dataset_with_features = preprocessing.timeline_feature_extraction(dataset, holidays)\n",
    "dataset_with_features.loc[\n",
    "    dataset_with_features[\"background\"] == 0, \"background\"\n",
    "] = \"Empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split datasets to train a machine learning tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FGZj9TyyTk8"
   },
   "outputs": [],
   "source": [
    "# train - test dataset split\n",
    "train_test_datasets = preprocessing.timeline_train_test_split(\n",
    "    dataset_with_features,\n",
    "    start_test=start_test,\n",
    ")\n",
    "\n",
    "train_dataset, test_dataset = train_test_datasets\n",
    "X_train, y_train = data.split_dataframe(train_dataset, target=\"validations\")\n",
    "X_test, y_test = data.split_dataframe(test_dataset, target=\"validations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxRu24uH8vQM"
   },
   "source": [
    "#### Define the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wjqVABVyTk8"
   },
   "outputs": [],
   "source": [
    "transformer = spg.make_column_transformer()\n",
    "\n",
    "# default regressor is interchangable with others like LinearRegression\n",
    "regressor = model.get_default_regressor(random_state=RANDOM_STATE)\n",
    "\n",
    "# k corresponds to the number of features to retain after encoding ~80 %\n",
    "regressor_pipeline = model.make_pipeline(\n",
    "    transformer,\n",
    "    regressor,\n",
    "    # k=48,\n",
    "    # random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UORhj0JelLX3"
   },
   "source": [
    "#### Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLPRegressor requires a 'np 1d array' for 'y' but not all regressors do it\n",
    "regressor_pipeline.fit(X_train, y_train.to_numpy().ravel())\n",
    "\n",
    "# setup a dataframe with results of training\n",
    "protected_results = model.predict_and_compare(regressor_pipeline, X_test, y_test)\n",
    "\n",
    "plot.residuals_plot(\n",
    "    protected_results,\n",
    "    attributes=(\"predictions\", \"residuals\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaQUkM6X8_2s"
   },
   "source": [
    "### Comparing the neural network against with original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.merge_dataframes(classes_dataset, buses_dataset)\n",
    "dataset_with_features = preprocessing.timeline_feature_extraction(dataset, holidays)\n",
    "dataset_with_features.loc[\n",
    "    dataset_with_features[\"background\"] == 0, \"background\"\n",
    "] = \"Empty\"\n",
    "\n",
    "train_test_datasets = preprocessing.timeline_train_test_split(\n",
    "    dataset_with_features,\n",
    "    start_test=start_test,\n",
    ")\n",
    "\n",
    "train_dataset, test_dataset = train_test_datasets\n",
    "X_train, y_train = data.split_dataframe(train_dataset, target=\"validations\")\n",
    "X_test, y_test = data.split_dataframe(test_dataset, target=\"validations\")\n",
    "\n",
    "regressor_2 = model.get_default_regressor(random_state=RANDOM_STATE)\n",
    "\n",
    "regressor_pipeline_2 = model.make_pipeline(\n",
    "    transformer,\n",
    "    regressor,\n",
    "    # k=48,\n",
    "    # random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "regressor_pipeline_2.fit(X_train, y_train.to_numpy().ravel())\n",
    "\n",
    "# setup a dataframe with results of training\n",
    "results = model.predict_and_compare(regressor_pipeline_2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8nCaMtem6sC"
   },
   "source": [
    " ### Step 2 (COMPARE): Visualize the predictions of the two models\n",
    " <a id='step_2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNEy9gZNyTk8"
   },
   "outputs": [],
   "source": [
    "timeline.predictions_interval_plot(\n",
    "    dataset,\n",
    "    protected_results,\n",
    "    results,\n",
    "    names=[\"GT\", \"FPA\", \"RAW\"],\n",
    ")\n",
    "\n",
    "####################\n",
    "# BEGIN : Observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END : Observe\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to try with other values ? Go back to [playing with the parameters](#playwithdp). \n",
    "\n",
    "__Food for thoughts__\n",
    "```\n",
    "####################\n",
    "# BEGIN : Answer\n",
    "```\n",
    "\n",
    "- How is the quality of the prediction impacted by the parameters ? You need to try several values in order to get a correct intuition. \n",
    "\n",
    "```\n",
    "# END : Answer\n",
    "####################\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Vibhor Rastogi and Suman Nath. Differentially private aggregation of distributed time-series with transformation and encryption. Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, June 2010, Indianapolis (IN) USA [[DOI]](https://doi.org/10.1145/1807167.1807247)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NmKSCNfpfOQ-",
    "qzLkzFP3fZm-",
    "O4teYxHrfnuh",
    "-kXTLBmBfxuL",
    "ZV4MJqgogTB0",
    "F-CFufprhULZ",
    "b5Hiwx5O7gXw",
    "tawuGE-n9PTw",
    "s4gBkGhqpaL7"
   ],
   "include_colab_link": true,
   "name": "a2r2-notebook03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "07e8c18849bc3008fa8e563e061106758a45cf3d8ca88585245492e31e786667"
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit ('venv-a2r2': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
